{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# import data\n",
    "\n",
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X\n",
    "\n",
    "X_train_sequence = open_pickle('../data/imdb/X_tr_sample_original.pkl')\n",
    "X_test_sequence = open_pickle('../data/imdb/X_te_sample_original.pkl')\n",
    "y_train_target = open_pickle('../data/imdb/y_tr_target_original.pkl')\n",
    "y_test_target = open_pickle('../data/imdb/y_te_target_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = open_pickle('../data/imdb_sequence/3000_one_hot/X_tr_seq_set.pkl')\n",
    "# y = open_pickle('../data/imdb_sequence/3000_one_hot/y_tr_seq_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3445910330077896616\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "\n",
    "#### One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NUM_WORDS = 1000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_train_sequence)\n",
    "\n",
    "num_encoder_tokens = tokenizer.num_words\n",
    "num_decoder_tokens = tokenizer.num_words\n",
    "max_encoder_seq_length = 81\n",
    "max_decoder_seq_length = 81\n",
    "\n",
    "# Input (X, y) needs to be raw sentences\n",
    "\n",
    "def generate_one_hot(X, y, X_test=None, y_test=None):\n",
    "    X_tr_padded = pad_sequences(tokenizer.texts_to_sequences(X), maxlen=81, padding='post', truncating='post')\n",
    "    y_tr_padded = pad_sequences(tokenizer.texts_to_sequences(y), maxlen=5, padding='post', truncating='post')\n",
    "    \n",
    "    # Variable init\n",
    "    encoder_input_data = np.zeros(\n",
    "        (len(X), max_encoder_seq_length, num_encoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_input_data = np.zeros(\n",
    "        (len(X), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "    decoder_target_data = np.zeros(\n",
    "        (len(y), max_decoder_seq_length, num_decoder_tokens),\n",
    "        dtype='float32')\n",
    "\n",
    "    # Generate sequence\n",
    "    for i, (input_text, target_text) in enumerate(zip(X_tr_padded, y_tr_padded)):\n",
    "        for t, word in enumerate(input_text):\n",
    "            encoder_input_data[i, t, word] = 1.\n",
    "\n",
    "        for t, word in enumerate(target_text):\n",
    "            # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "            decoder_input_data[i, t, word] = 1.\n",
    "\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                decoder_target_data[i, t - 1, word] = 1.\n",
    "                \n",
    "    if X_test is not None and y_test is not None:\n",
    "        X_te_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=81, padding='post', truncating='post')\n",
    "        y_te_padded = pad_sequences(tokenizer.texts_to_sequences(y_test), maxlen=5, padding='post', truncating='post')\n",
    "        \n",
    "        return X_tr_padded, y_tr_padded, X_te_padded, y_te_padded, encoder_input_data, decoder_input_data, decoder_target_data\n",
    "    else:\n",
    "        return X_tr_padded, y_tr_padded, encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr_padded, y_tr_padded, encoder_input_data, decoder_input_data, decoder_target_data = generate_one_hot(X_train_sequence, y_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = {\n",
    "#     'text': X_train_sequence,\n",
    "#     'padded' : X_tr_padded\n",
    "# }\n",
    "\n",
    "\n",
    "# X_test = {\n",
    "#     'text' : X_test_sequence,\n",
    "#     'padded' : X_te_padded\n",
    "# }\n",
    "\n",
    "# y_train = {\n",
    "#     'text': y_train_target,\n",
    "#     'padded' : y_tr_padded\n",
    "# }\n",
    "\n",
    "# y_test = {\n",
    "#     'text' : y_test_target,\n",
    "#     'padded' : y_te_padded\n",
    "# }\n",
    "\n",
    "# X_tr_encoder_input = {\n",
    "#     'encoder_input' : encoder_input_data\n",
    "# }\n",
    "\n",
    "# y_tr_decoder = {\n",
    "#     'decoder_input' : decoder_input_data,\n",
    "#     'decoder_target' : decoder_target_data\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pickle.dump(X_train, open('../data/imdb_sequence/1000_one_hot/X_tr_seq_set.pkl','wb'))\n",
    "# pickle.dump(y_train, open('../data/imdb_sequence/1000_one_hot/y_tr_seq_set.pkl','wb'))\n",
    "\n",
    "# # pickle.dump(X_tr_encoder_input, open('../data/imdb_sequence/1000_one_hot/X_tr_encoder.pkl','wb'))\n",
    "# # pickle.dump(y_tr_decoder , open('../data/imdb_sequence/1000_one_hot/y_tr_decoder.pkl','wb'))\n",
    "\n",
    "\n",
    "# pickle.dump(X_test, open('../data/imdb_sequence/1000_one_hot/X_te_seq_set.pkl','wb'))\n",
    "# pickle.dump(y_test, open('../data/imdb_sequence/1000_one_hot/y_te_seq_set.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = \"../data/glove.6B/\"\n",
    "GLOVE_DIM = 100\n",
    "\n",
    "def extract_glove_index(file):\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(GLOVE_DIR, file), 'r')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = extract_glove_index('glove.6B.100d.txt')\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.33979  ,  0.20941  ,  0.46348  , -0.64792  , -0.38377  ,\n",
       "        0.038034 ,  0.17127  ,  0.15978  ,  0.46619  , -0.019169 ,\n",
       "        0.41479  , -0.34349  ,  0.26872  ,  0.04464  ,  0.42131  ,\n",
       "       -0.41032  ,  0.15459  ,  0.022239 , -0.64653  ,  0.25256  ,\n",
       "        0.043136 , -0.19445  ,  0.46516  ,  0.45651  ,  0.68588  ,\n",
       "        0.091295 ,  0.21875  , -0.70351  ,  0.16785  , -0.35079  ,\n",
       "       -0.12634  ,  0.66384  , -0.2582   ,  0.036542 , -0.13605  ,\n",
       "        0.40253  ,  0.14289  ,  0.38132  , -0.12283  , -0.45886  ,\n",
       "       -0.25282  , -0.30432  , -0.11215  , -0.26182  , -0.22482  ,\n",
       "       -0.44554  ,  0.2991   , -0.85612  , -0.14503  , -0.49086  ,\n",
       "        0.0082973, -0.17491  ,  0.27524  ,  1.4401   , -0.21239  ,\n",
       "       -2.8435   , -0.27958  , -0.45722  ,  1.6386   ,  0.78808  ,\n",
       "       -0.55262  ,  0.65     ,  0.086426 ,  0.39012  ,  1.0632   ,\n",
       "       -0.35379  ,  0.48328  ,  0.346    ,  0.84174  ,  0.098707 ,\n",
       "       -0.24213  , -0.27053  ,  0.045287 , -0.40147  ,  0.11395  ,\n",
       "        0.0062226,  0.036673 ,  0.018518 , -1.0213   , -0.20806  ,\n",
       "        0.64072  , -0.068763 , -0.58635  ,  0.33476  , -1.1432   ,\n",
       "       -0.1148   , -0.25091  , -0.45907  , -0.096819 , -0.17946  ,\n",
       "       -0.063351 , -0.67412  , -0.068895 ,  0.53604  , -0.87773  ,\n",
       "        0.31802  , -0.39242  , -0.23394  ,  0.47298  , -0.028803 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<UNK>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1447dc944dce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# embeddings_index['n\\'t']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membeddings_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<UNK>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '<UNK>'"
     ]
    }
   ],
   "source": [
    "# embeddings_index.keys()\n",
    "\n",
    "# embeddings_index['n\\'t']\n",
    "embeddings_index['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.9104e-01,  1.7601e-01,  3.6920e-01, -5.0323e-01, -4.7561e-01,\n",
       "        1.5798e-01, -1.1679e-01,  2.1052e-01,  3.2652e-01,  1.2194e-01,\n",
       "        9.0944e-02,  2.6089e-01,  7.6294e-01,  6.9673e-04, -5.0001e-02,\n",
       "       -4.4853e-01,  3.6239e-01,  5.6345e-01, -6.8702e-01,  3.3237e-01,\n",
       "        3.1285e-01, -1.4207e-01,  3.5327e-01, -1.6426e-01, -1.0693e-01,\n",
       "        7.7786e-02, -1.7704e-01, -9.2897e-01,  1.4680e-01, -1.3585e-01,\n",
       "        2.5682e-01,  6.6019e-01, -3.5569e-01,  2.1838e-01,  3.8173e-01,\n",
       "        5.4337e-01,  1.0197e-01,  3.5230e-01, -2.5510e-01, -1.5155e-01,\n",
       "       -6.7434e-01,  1.6903e-01,  1.6413e-01, -5.3843e-01, -1.7457e-01,\n",
       "       -2.8539e-01,  7.4044e-01, -6.7533e-01, -2.3382e-01, -1.3599e+00,\n",
       "        3.0225e-01, -1.4968e-01,  2.7043e-01,  1.1979e+00, -2.9556e-01,\n",
       "       -2.5395e+00,  1.0303e-03, -2.6272e-01,  1.8303e+00,  8.0008e-01,\n",
       "       -3.5691e-01,  5.6578e-01, -5.5040e-01,  7.0845e-02,  1.4275e+00,\n",
       "        9.0160e-02,  7.8420e-01,  7.8490e-01, -3.3538e-01, -6.5751e-01,\n",
       "       -2.0112e-01, -1.0297e+00,  6.9195e-02, -6.1272e-01,  1.1373e-01,\n",
       "       -1.9547e-01, -2.1256e-01,  4.9763e-02, -1.1619e+00, -6.4512e-02,\n",
       "        5.3146e-01, -4.7384e-01, -6.8709e-01,  1.3024e-01, -2.0899e+00,\n",
       "       -4.1346e-01,  3.0364e-01, -5.7448e-04, -1.8833e-01, -5.4779e-01,\n",
       "       -3.2058e-01, -3.6704e-01, -1.4740e-01, -1.9044e-01, -4.7712e-01,\n",
       "        4.8228e-02, -2.6215e-01, -5.9680e-01,  8.0843e-02,  2.7866e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['not']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 81\n",
    "max_decoder_seq_length = 5\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_sequence)\n",
    "\n",
    "X_tr_padded = pad_sequences(tokenizer.texts_to_sequences(X_train_sequence), maxlen=81, padding='post', truncating='post')\n",
    "y_tr_padded = pad_sequences(tokenizer.texts_to_sequences(y_train_target), maxlen=5, padding='post', truncating='post')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(X_train_sequence), max_encoder_seq_length, GLOVE_DIM),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(y_train_target), max_decoder_seq_length, GLOVE_DIM),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(y_train_target), max_decoder_seq_length, len(tokenizer.word_index)),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = []\n",
    "target_sequence = []\n",
    "\n",
    "for sample in X_train_sequence:\n",
    "    train_sequence.append(sample.split())\n",
    "for target in y_train_target:\n",
    "    target_sequence.append(target.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlen = [len(seq) for seq in target_sequence]\n",
    "seqlen = np.asarray(seqlen)\n",
    "np.max(seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([70, 70, 70, ..., 70, 70, 70])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, '<pad>', 1, 2, 3, 4]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "\n",
    "padding_index = (seqlen < 81).astype(int)\n",
    "a = [1,2,3,4]\n",
    "\n",
    "a.insert(0, '<pad>')\n",
    "a.insert(0, 0)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42356"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(y_tr_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100-dim -> input sequence, input decoder\n",
    "# 42K-dim -> output sequence.\n",
    "\n",
    "for i, (input_text, target_text, target_padded) in enumerate(zip(train_sequence, target_sequence, y_tr_padded)):\n",
    "    for t, word in enumerate(input_text):\n",
    "        try:\n",
    "            encoder_input_data[i, t, :] = embeddings_index[word]\n",
    "        except KeyError as error:\n",
    "            continue\n",
    "    \n",
    "    for t, word in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        try:\n",
    "            decoder_input_data[i, t, :] = embeddings_index[word]\n",
    "        except KeyError as error:\n",
    "            continue\n",
    "        \n",
    "    for t, word in enumerate(target_padded):\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, word] = 1.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22752, 81, 1000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(t.word_docs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = X_tr_padded.shape[1]\n",
    "max_decoder_seq_length = y_tr_padded.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 22752\n",
      "Number of unique input tokens: 3000\n",
      "Number of unique output tokens: 3000\n",
      "Max sequence length for inputs: 85\n",
      "Max sequence length for outputs: 5\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(X_tr_sequence))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22752, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder - decoder here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(X_tr_sequence), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(y_tr_sequence), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(y_tr_sequence), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(X_tr_padded, y_tr_padded)):\n",
    "    for t, word in enumerate(input_text):\n",
    "        encoder_input_data[i, t, word] = 1.\n",
    "        \n",
    "    for t, word in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, word] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, word] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18201 samples, validate on 4551 samples\n",
      "Epoch 1/500\n",
      "18201/18201 [==============================] - 61s 3ms/step - loss: 4.7087 - val_loss: 4.2728\n",
      "Epoch 2/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 4.1975 - val_loss: 4.1529\n",
      "Epoch 3/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 4.0842 - val_loss: 4.0701\n",
      "Epoch 4/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 4.0066 - val_loss: 4.0062\n",
      "Epoch 5/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.9520 - val_loss: 3.9620\n",
      "Epoch 6/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.9069 - val_loss: 3.9292\n",
      "Epoch 7/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.8667 - val_loss: 3.8924\n",
      "Epoch 8/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.8275 - val_loss: 3.8614\n",
      "Epoch 9/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.7850 - val_loss: 3.8218\n",
      "Epoch 10/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.7416 - val_loss: 3.7881\n",
      "Epoch 11/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.7010 - val_loss: 3.7561\n",
      "Epoch 12/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.6625 - val_loss: 3.7254\n",
      "Epoch 13/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.6277 - val_loss: 3.6946\n",
      "Epoch 14/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.5968 - val_loss: 3.6729\n",
      "Epoch 15/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.5697 - val_loss: 3.6543\n",
      "Epoch 16/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.5433 - val_loss: 3.6303\n",
      "Epoch 17/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.5180 - val_loss: 3.6166\n",
      "Epoch 18/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.4953 - val_loss: 3.5962\n",
      "Epoch 19/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.4730 - val_loss: 3.5778\n",
      "Epoch 20/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.4523 - val_loss: 3.5686\n",
      "Epoch 21/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.4338 - val_loss: 3.5507\n",
      "Epoch 22/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.4165 - val_loss: 3.5375\n",
      "Epoch 23/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.3978 - val_loss: 3.5266\n",
      "Epoch 24/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.3804 - val_loss: 3.5184\n",
      "Epoch 25/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.3645 - val_loss: 3.5045\n",
      "Epoch 26/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.3482 - val_loss: 3.4960\n",
      "Epoch 27/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.3330 - val_loss: 3.4860\n",
      "Epoch 28/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.3179 - val_loss: 3.4769\n",
      "Epoch 29/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 3.3035 - val_loss: 3.4686\n",
      "Epoch 30/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2902 - val_loss: 3.4623\n",
      "Epoch 31/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2770 - val_loss: 3.4600\n",
      "Epoch 32/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2653 - val_loss: 3.4485\n",
      "Epoch 33/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2533 - val_loss: 3.4488\n",
      "Epoch 34/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2411 - val_loss: 3.4371\n",
      "Epoch 35/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2288 - val_loss: 3.4319\n",
      "Epoch 36/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2174 - val_loss: 3.4296\n",
      "Epoch 37/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.2062 - val_loss: 3.4301\n",
      "Epoch 38/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1963 - val_loss: 3.4252\n",
      "Epoch 39/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 3.1868 - val_loss: 3.4170\n",
      "Epoch 40/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1761 - val_loss: 3.4163\n",
      "Epoch 41/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1678 - val_loss: 3.4147\n",
      "Epoch 42/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1583 - val_loss: 3.4038\n",
      "Epoch 43/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 3.1502 - val_loss: 3.4035\n",
      "Epoch 44/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1398 - val_loss: 3.4025\n",
      "Epoch 45/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1314 - val_loss: 3.3995\n",
      "Epoch 46/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1229 - val_loss: 3.3976\n",
      "Epoch 47/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1145 - val_loss: 3.3945\n",
      "Epoch 48/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.1082 - val_loss: 3.3930\n",
      "Epoch 49/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0991 - val_loss: 3.3916\n",
      "Epoch 50/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 3.0922 - val_loss: 3.3908\n",
      "Epoch 51/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 3.0854 - val_loss: 3.3943\n",
      "Epoch 52/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0773 - val_loss: 3.3880\n",
      "Epoch 53/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 3.0701 - val_loss: 3.3879\n",
      "Epoch 54/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0631 - val_loss: 3.3885\n",
      "Epoch 55/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0560 - val_loss: 3.3865\n",
      "Epoch 56/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0504 - val_loss: 3.3854\n",
      "Epoch 57/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0443 - val_loss: 3.3917\n",
      "Epoch 58/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0369 - val_loss: 3.3929\n",
      "Epoch 59/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0315 - val_loss: 3.3878\n",
      "Epoch 60/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0240 - val_loss: 3.3861\n",
      "Epoch 61/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0189 - val_loss: 3.3861\n",
      "Epoch 62/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0119 - val_loss: 3.3824\n",
      "Epoch 63/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0066 - val_loss: 3.3870\n",
      "Epoch 64/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 3.0025 - val_loss: 3.3906\n",
      "Epoch 65/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9957 - val_loss: 3.3900\n",
      "Epoch 66/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9905 - val_loss: 3.3890\n",
      "Epoch 67/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9845 - val_loss: 3.3944\n",
      "Epoch 68/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9796 - val_loss: 3.3919\n",
      "Epoch 69/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9747 - val_loss: 3.3918\n",
      "Epoch 70/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9695 - val_loss: 3.3943\n",
      "Epoch 71/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9649 - val_loss: 3.3956\n",
      "Epoch 72/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9593 - val_loss: 3.3983\n",
      "Epoch 73/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.9543 - val_loss: 3.4005\n",
      "Epoch 74/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.9514 - val_loss: 3.3963\n",
      "Epoch 75/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.9445 - val_loss: 3.3984\n",
      "Epoch 76/500\n",
      "18201/18201 [==============================] - 59s 3ms/step - loss: 2.9402 - val_loss: 3.4032\n",
      "Epoch 77/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9345 - val_loss: 3.3997\n",
      "Epoch 78/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9308 - val_loss: 3.4010\n",
      "Epoch 79/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.9267 - val_loss: 3.4026\n",
      "Epoch 80/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9225 - val_loss: 3.4118\n",
      "Epoch 81/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9203 - val_loss: 3.4109\n",
      "Epoch 82/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.9141 - val_loss: 3.4112\n",
      "Epoch 83/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.9113 - val_loss: 3.4098\n",
      "Epoch 84/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.9056 - val_loss: 3.4109\n",
      "Epoch 85/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.9016 - val_loss: 3.4105\n",
      "Epoch 86/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8974 - val_loss: 3.4174\n",
      "Epoch 87/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8931 - val_loss: 3.4172\n",
      "Epoch 88/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8897 - val_loss: 3.4162\n",
      "Epoch 89/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8849 - val_loss: 3.4184\n",
      "Epoch 90/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8808 - val_loss: 3.4233\n",
      "Epoch 91/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8783 - val_loss: 3.4233\n",
      "Epoch 92/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8733 - val_loss: 3.4262\n",
      "Epoch 93/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8686 - val_loss: 3.4265\n",
      "Epoch 94/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8664 - val_loss: 3.4273\n",
      "Epoch 95/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8630 - val_loss: 3.4317\n",
      "Epoch 96/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8589 - val_loss: 3.4338\n",
      "Epoch 97/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8552 - val_loss: 3.4355\n",
      "Epoch 98/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8508 - val_loss: 3.4385\n",
      "Epoch 99/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8467 - val_loss: 3.4373\n",
      "Epoch 100/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8420 - val_loss: 3.4422\n",
      "Epoch 101/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8400 - val_loss: 3.4422\n",
      "Epoch 102/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8358 - val_loss: 3.4501\n",
      "Epoch 103/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8315 - val_loss: 3.4530\n",
      "Epoch 104/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8283 - val_loss: 3.4474\n",
      "Epoch 105/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.8246 - val_loss: 3.4538\n",
      "Epoch 106/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8209 - val_loss: 3.4561\n",
      "Epoch 107/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8177 - val_loss: 3.4579\n",
      "Epoch 108/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8154 - val_loss: 3.4606\n",
      "Epoch 109/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8107 - val_loss: 3.4655\n",
      "Epoch 110/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8071 - val_loss: 3.4708\n",
      "Epoch 111/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.8040 - val_loss: 3.4700\n",
      "Epoch 112/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7997 - val_loss: 3.4736\n",
      "Epoch 113/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7964 - val_loss: 3.4787\n",
      "Epoch 114/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7939 - val_loss: 3.4818\n",
      "Epoch 115/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7902 - val_loss: 3.4809\n",
      "Epoch 116/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7861 - val_loss: 3.4878\n",
      "Epoch 117/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7844 - val_loss: 3.4895\n",
      "Epoch 118/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7801 - val_loss: 3.4886\n",
      "Epoch 119/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7774 - val_loss: 3.4942\n",
      "Epoch 120/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7752 - val_loss: 3.4953\n",
      "Epoch 121/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7730 - val_loss: 3.5032\n",
      "Epoch 122/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7679 - val_loss: 3.5032\n",
      "Epoch 123/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7650 - val_loss: 3.5119\n",
      "Epoch 124/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7604 - val_loss: 3.5093\n",
      "Epoch 125/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7576 - val_loss: 3.5117\n",
      "Epoch 126/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7551 - val_loss: 3.5177\n",
      "Epoch 127/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7523 - val_loss: 3.5235\n",
      "Epoch 128/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7509 - val_loss: 3.5272\n",
      "Epoch 129/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7475 - val_loss: 3.5250\n",
      "Epoch 130/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7443 - val_loss: 3.5300\n",
      "Epoch 131/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7409 - val_loss: 3.5312\n",
      "Epoch 132/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7393 - val_loss: 3.5369\n",
      "Epoch 133/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7364 - val_loss: 3.5404\n",
      "Epoch 134/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7346 - val_loss: 3.5463\n",
      "Epoch 135/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7309 - val_loss: 3.5490\n",
      "Epoch 136/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7293 - val_loss: 3.5507\n",
      "Epoch 137/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7263 - val_loss: 3.5528\n",
      "Epoch 138/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.7234 - val_loss: 3.5529\n",
      "Epoch 139/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7186 - val_loss: 3.5578\n",
      "Epoch 140/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7164 - val_loss: 3.5643\n",
      "Epoch 141/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7138 - val_loss: 3.5703\n",
      "Epoch 142/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7111 - val_loss: 3.5706\n",
      "Epoch 143/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7110 - val_loss: 3.5738\n",
      "Epoch 144/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7080 - val_loss: 3.5782\n",
      "Epoch 145/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7055 - val_loss: 3.5793\n",
      "Epoch 146/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7036 - val_loss: 3.5833\n",
      "Epoch 147/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.7007 - val_loss: 3.5840\n",
      "Epoch 148/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6988 - val_loss: 3.5897\n",
      "Epoch 149/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6951 - val_loss: 3.5883\n",
      "Epoch 150/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6940 - val_loss: 3.5967\n",
      "Epoch 151/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6913 - val_loss: 3.5961\n",
      "Epoch 152/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6884 - val_loss: 3.6010\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6873 - val_loss: 3.6039\n",
      "Epoch 154/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6849 - val_loss: 3.6116\n",
      "Epoch 155/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6827 - val_loss: 3.6133\n",
      "Epoch 156/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6788 - val_loss: 3.6179\n",
      "Epoch 157/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6778 - val_loss: 3.6197\n",
      "Epoch 158/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6753 - val_loss: 3.6245\n",
      "Epoch 159/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6727 - val_loss: 3.6306\n",
      "Epoch 160/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6692 - val_loss: 3.6377\n",
      "Epoch 161/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6720 - val_loss: 3.6333\n",
      "Epoch 162/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6670 - val_loss: 3.6316\n",
      "Epoch 163/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6640 - val_loss: 3.6355\n",
      "Epoch 164/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6604 - val_loss: 3.6415\n",
      "Epoch 165/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6586 - val_loss: 3.6521\n",
      "Epoch 166/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6584 - val_loss: 3.6471\n",
      "Epoch 167/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6539 - val_loss: 3.6541\n",
      "Epoch 168/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6525 - val_loss: 3.6605\n",
      "Epoch 169/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6493 - val_loss: 3.6566\n",
      "Epoch 170/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6492 - val_loss: 3.6632\n",
      "Epoch 171/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6456 - val_loss: 3.6658\n",
      "Epoch 172/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6428 - val_loss: 3.6636\n",
      "Epoch 173/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6416 - val_loss: 3.6773\n",
      "Epoch 174/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6402 - val_loss: 3.6765\n",
      "Epoch 175/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6364 - val_loss: 3.6778\n",
      "Epoch 176/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6342 - val_loss: 3.6801\n",
      "Epoch 177/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6368 - val_loss: 3.6876\n",
      "Epoch 178/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6325 - val_loss: 3.6874\n",
      "Epoch 179/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6279 - val_loss: 3.6862\n",
      "Epoch 180/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6259 - val_loss: 3.6985\n",
      "Epoch 181/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6222 - val_loss: 3.7022\n",
      "Epoch 182/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6198 - val_loss: 3.6994\n",
      "Epoch 183/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6163 - val_loss: 3.7051\n",
      "Epoch 184/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6161 - val_loss: 3.7089\n",
      "Epoch 185/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6129 - val_loss: 3.7054\n",
      "Epoch 186/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6115 - val_loss: 3.7086\n",
      "Epoch 187/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6079 - val_loss: 3.7197\n",
      "Epoch 188/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6054 - val_loss: 3.7153\n",
      "Epoch 189/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6054 - val_loss: 3.7145\n",
      "Epoch 190/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6039 - val_loss: 3.7197\n",
      "Epoch 191/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.6001 - val_loss: 3.7251\n",
      "Epoch 192/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5958 - val_loss: 3.7281\n",
      "Epoch 193/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5930 - val_loss: 3.7324\n",
      "Epoch 194/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5922 - val_loss: 3.7248\n",
      "Epoch 195/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5944 - val_loss: 3.7351\n",
      "Epoch 196/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5903 - val_loss: 3.7454\n",
      "Epoch 197/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5850 - val_loss: 3.7464\n",
      "Epoch 198/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5819 - val_loss: 3.7543\n",
      "Epoch 199/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5791 - val_loss: 3.7512\n",
      "Epoch 200/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5788 - val_loss: 3.7514\n",
      "Epoch 201/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5772 - val_loss: 3.7495\n",
      "Epoch 202/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5735 - val_loss: 3.7636\n",
      "Epoch 203/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5703 - val_loss: 3.7709\n",
      "Epoch 204/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5684 - val_loss: 3.7717\n",
      "Epoch 205/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5672 - val_loss: 3.7720\n",
      "Epoch 206/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5656 - val_loss: 3.7759\n",
      "Epoch 207/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5611 - val_loss: 3.7772\n",
      "Epoch 208/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5591 - val_loss: 3.7800\n",
      "Epoch 209/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5582 - val_loss: 3.7901\n",
      "Epoch 210/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5547 - val_loss: 3.7837\n",
      "Epoch 211/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5498 - val_loss: 3.7913\n",
      "Epoch 212/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5482 - val_loss: 3.7936\n",
      "Epoch 213/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5453 - val_loss: 3.7983\n",
      "Epoch 214/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5416 - val_loss: 3.7963\n",
      "Epoch 215/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5385 - val_loss: 3.8104\n",
      "Epoch 216/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5397 - val_loss: 3.8131\n",
      "Epoch 217/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5356 - val_loss: 3.8149\n",
      "Epoch 218/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5309 - val_loss: 3.8122\n",
      "Epoch 219/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5278 - val_loss: 3.8153\n",
      "Epoch 220/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5261 - val_loss: 3.8168\n",
      "Epoch 221/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5210 - val_loss: 3.8317\n",
      "Epoch 222/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5196 - val_loss: 3.8313\n",
      "Epoch 223/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5162 - val_loss: 3.8350\n",
      "Epoch 224/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5157 - val_loss: 3.8343\n",
      "Epoch 225/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5100 - val_loss: 3.8346\n",
      "Epoch 226/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5054 - val_loss: 3.8406\n",
      "Epoch 227/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5026 - val_loss: 3.8450\n",
      "Epoch 228/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.5008 - val_loss: 3.8462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4968 - val_loss: 3.8519\n",
      "Epoch 230/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4931 - val_loss: 3.8592\n",
      "Epoch 231/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4910 - val_loss: 3.8655\n",
      "Epoch 232/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4885 - val_loss: 3.8567\n",
      "Epoch 233/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4857 - val_loss: 3.8674\n",
      "Epoch 234/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4824 - val_loss: 3.8738\n",
      "Epoch 235/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4774 - val_loss: 3.8750\n",
      "Epoch 236/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4757 - val_loss: 3.8820\n",
      "Epoch 237/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4751 - val_loss: 3.8865\n",
      "Epoch 238/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4698 - val_loss: 3.8895\n",
      "Epoch 239/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4684 - val_loss: 3.8833\n",
      "Epoch 240/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4648 - val_loss: 3.8906\n",
      "Epoch 241/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4598 - val_loss: 3.8945\n",
      "Epoch 242/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4565 - val_loss: 3.9128\n",
      "Epoch 243/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4542 - val_loss: 3.8974\n",
      "Epoch 244/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4510 - val_loss: 3.9038\n",
      "Epoch 245/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4481 - val_loss: 3.9087\n",
      "Epoch 246/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4460 - val_loss: 3.9215\n",
      "Epoch 247/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4416 - val_loss: 3.9115\n",
      "Epoch 248/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4405 - val_loss: 3.9181\n",
      "Epoch 249/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4380 - val_loss: 3.9264\n",
      "Epoch 250/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4361 - val_loss: 3.9236\n",
      "Epoch 251/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4330 - val_loss: 3.9287\n",
      "Epoch 252/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4314 - val_loss: 3.9347\n",
      "Epoch 253/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4263 - val_loss: 3.9314\n",
      "Epoch 254/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4203 - val_loss: 3.9316\n",
      "Epoch 255/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4175 - val_loss: 3.9386\n",
      "Epoch 256/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4138 - val_loss: 3.9449\n",
      "Epoch 257/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4117 - val_loss: 3.9606\n",
      "Epoch 258/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4109 - val_loss: 3.9595\n",
      "Epoch 259/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4089 - val_loss: 3.9560\n",
      "Epoch 260/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4061 - val_loss: 3.9628\n",
      "Epoch 261/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.4021 - val_loss: 3.9615\n",
      "Epoch 262/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3968 - val_loss: 3.9761\n",
      "Epoch 263/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3944 - val_loss: 3.9808\n",
      "Epoch 264/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3913 - val_loss: 3.9800\n",
      "Epoch 265/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3890 - val_loss: 3.9864\n",
      "Epoch 266/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3878 - val_loss: 3.9801\n",
      "Epoch 267/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3858 - val_loss: 3.9929\n",
      "Epoch 268/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3813 - val_loss: 3.9887\n",
      "Epoch 269/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3814 - val_loss: 3.9970\n",
      "Epoch 270/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3771 - val_loss: 3.9975\n",
      "Epoch 271/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3728 - val_loss: 4.0069\n",
      "Epoch 272/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3695 - val_loss: 4.0151\n",
      "Epoch 273/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3666 - val_loss: 4.0101\n",
      "Epoch 274/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3632 - val_loss: 4.0167\n",
      "Epoch 275/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3622 - val_loss: 4.0248\n",
      "Epoch 276/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3579 - val_loss: 4.0257\n",
      "Epoch 277/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3562 - val_loss: 4.0280\n",
      "Epoch 278/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3532 - val_loss: 4.0416\n",
      "Epoch 279/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3510 - val_loss: 4.0341\n",
      "Epoch 280/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3489 - val_loss: 4.0387\n",
      "Epoch 281/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3475 - val_loss: 4.0497\n",
      "Epoch 282/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3431 - val_loss: 4.0461\n",
      "Epoch 283/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3398 - val_loss: 4.0538\n",
      "Epoch 284/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3374 - val_loss: 4.0610\n",
      "Epoch 285/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3343 - val_loss: 4.0601\n",
      "Epoch 286/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3317 - val_loss: 4.0612\n",
      "Epoch 287/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3298 - val_loss: 4.0695\n",
      "Epoch 288/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3262 - val_loss: 4.0808\n",
      "Epoch 289/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3244 - val_loss: 4.0834\n",
      "Epoch 290/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.3209 - val_loss: 4.0843\n",
      "Epoch 291/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3171 - val_loss: 4.0860\n",
      "Epoch 292/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3152 - val_loss: 4.0901\n",
      "Epoch 293/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3198 - val_loss: 4.0933\n",
      "Epoch 294/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3116 - val_loss: 4.0925\n",
      "Epoch 295/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3090 - val_loss: 4.0958\n",
      "Epoch 296/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3050 - val_loss: 4.0995\n",
      "Epoch 297/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3029 - val_loss: 4.1013\n",
      "Epoch 298/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.3014 - val_loss: 4.1112\n",
      "Epoch 299/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2956 - val_loss: 4.1143\n",
      "Epoch 300/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2905 - val_loss: 4.1222\n",
      "Epoch 301/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2895 - val_loss: 4.1250\n",
      "Epoch 302/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2905 - val_loss: 4.1204\n",
      "Epoch 303/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2851 - val_loss: 4.1298\n",
      "Epoch 304/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2855 - val_loss: 4.1297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2807 - val_loss: 4.1385\n",
      "Epoch 306/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2763 - val_loss: 4.1447\n",
      "Epoch 307/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.2734 - val_loss: 4.1503\n",
      "Epoch 308/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2705 - val_loss: 4.1477\n",
      "Epoch 309/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2687 - val_loss: 4.1510\n",
      "Epoch 310/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2669 - val_loss: 4.1586\n",
      "Epoch 311/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2634 - val_loss: 4.1671\n",
      "Epoch 312/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2618 - val_loss: 4.1748\n",
      "Epoch 313/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2565 - val_loss: 4.1711\n",
      "Epoch 314/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2558 - val_loss: 4.1759\n",
      "Epoch 315/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2563 - val_loss: 4.1753\n",
      "Epoch 316/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2541 - val_loss: 4.1768\n",
      "Epoch 317/500\n",
      "18201/18201 [==============================] - 54s 3ms/step - loss: 2.2463 - val_loss: 4.1887\n",
      "Epoch 318/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.2445 - val_loss: 4.1889\n",
      "Epoch 319/500\n",
      "18201/18201 [==============================] - 55s 3ms/step - loss: 2.2430 - val_loss: 4.2051\n",
      "Epoch 320/500\n",
      "18201/18201 [==============================] - 60s 3ms/step - loss: 2.2456 - val_loss: 4.1945\n",
      "Epoch 321/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.2370 - val_loss: 4.2143\n",
      "Epoch 322/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2345 - val_loss: 4.2124\n",
      "Epoch 323/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2314 - val_loss: 4.2187\n",
      "Epoch 324/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2320 - val_loss: 4.2133\n",
      "Epoch 325/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2298 - val_loss: 4.2275\n",
      "Epoch 326/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2271 - val_loss: 4.2224\n",
      "Epoch 327/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2242 - val_loss: 4.2397\n",
      "Epoch 328/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2253 - val_loss: 4.2230\n",
      "Epoch 329/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2211 - val_loss: 4.2306\n",
      "Epoch 330/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2156 - val_loss: 4.2374\n",
      "Epoch 331/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2111 - val_loss: 4.2451\n",
      "Epoch 332/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.2072 - val_loss: 4.2551\n",
      "Epoch 333/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2074 - val_loss: 4.2628\n",
      "Epoch 334/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2091 - val_loss: 4.2616\n",
      "Epoch 335/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2093 - val_loss: 4.2598\n",
      "Epoch 336/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.2020 - val_loss: 4.2638\n",
      "Epoch 337/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1949 - val_loss: 4.2651\n",
      "Epoch 338/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1928 - val_loss: 4.2848\n",
      "Epoch 339/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1902 - val_loss: 4.2776\n",
      "Epoch 340/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1894 - val_loss: 4.2809\n",
      "Epoch 341/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1854 - val_loss: 4.2823\n",
      "Epoch 342/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1829 - val_loss: 4.2952\n",
      "Epoch 343/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1802 - val_loss: 4.2967\n",
      "Epoch 344/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1802 - val_loss: 4.2968\n",
      "Epoch 345/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1791 - val_loss: 4.3039\n",
      "Epoch 346/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1737 - val_loss: 4.3108\n",
      "Epoch 347/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1766 - val_loss: 4.3101\n",
      "Epoch 348/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1703 - val_loss: 4.3157\n",
      "Epoch 349/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1649 - val_loss: 4.3208\n",
      "Epoch 350/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1642 - val_loss: 4.3236\n",
      "Epoch 351/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1635 - val_loss: 4.3314\n",
      "Epoch 352/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1589 - val_loss: 4.3275\n",
      "Epoch 353/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1542 - val_loss: 4.3385\n",
      "Epoch 354/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1517 - val_loss: 4.3392\n",
      "Epoch 355/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1501 - val_loss: 4.3485\n",
      "Epoch 356/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1483 - val_loss: 4.3428\n",
      "Epoch 357/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1472 - val_loss: 4.3566\n",
      "Epoch 358/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1505 - val_loss: 4.3472\n",
      "Epoch 359/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1427 - val_loss: 4.3511\n",
      "Epoch 360/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1381 - val_loss: 4.3626\n",
      "Epoch 361/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1327 - val_loss: 4.3680\n",
      "Epoch 362/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1308 - val_loss: 4.3808\n",
      "Epoch 363/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1291 - val_loss: 4.3761\n",
      "Epoch 364/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1299 - val_loss: 4.3744\n",
      "Epoch 365/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1245 - val_loss: 4.3873\n",
      "Epoch 366/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1237 - val_loss: 4.3924\n",
      "Epoch 367/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1216 - val_loss: 4.3984\n",
      "Epoch 368/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1183 - val_loss: 4.3986\n",
      "Epoch 369/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1154 - val_loss: 4.4044\n",
      "Epoch 370/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1173 - val_loss: 4.4019\n",
      "Epoch 371/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1126 - val_loss: 4.4172\n",
      "Epoch 372/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1082 - val_loss: 4.4210\n",
      "Epoch 373/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1030 - val_loss: 4.4192\n",
      "Epoch 374/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.1021 - val_loss: 4.4344\n",
      "Epoch 375/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1033 - val_loss: 4.4332\n",
      "Epoch 376/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1545 - val_loss: 4.4131\n",
      "Epoch 377/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1692 - val_loss: 4.4223\n",
      "Epoch 378/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.1046 - val_loss: 4.4219\n",
      "Epoch 379/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0948 - val_loss: 4.4380\n",
      "Epoch 380/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0925 - val_loss: 4.4408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 381/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0941 - val_loss: 4.4449\n",
      "Epoch 382/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0884 - val_loss: 4.4494\n",
      "Epoch 383/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0814 - val_loss: 4.4559\n",
      "Epoch 384/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0830 - val_loss: 4.4596\n",
      "Epoch 385/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.0807 - val_loss: 4.4629\n",
      "Epoch 386/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0750 - val_loss: 4.4682\n",
      "Epoch 387/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0725 - val_loss: 4.4735\n",
      "Epoch 388/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.0705 - val_loss: 4.4749\n",
      "Epoch 389/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0722 - val_loss: 4.4746\n",
      "Epoch 390/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0719 - val_loss: 4.4843\n",
      "Epoch 391/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0680 - val_loss: 4.4889\n",
      "Epoch 392/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.0613 - val_loss: 4.4909\n",
      "Epoch 393/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0586 - val_loss: 4.5045\n",
      "Epoch 394/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.0556 - val_loss: 4.5035\n",
      "Epoch 395/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0606 - val_loss: 4.5130\n",
      "Epoch 396/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.0564 - val_loss: 4.5170\n",
      "Epoch 397/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0517 - val_loss: 4.5258\n",
      "Epoch 398/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0501 - val_loss: 4.5265\n",
      "Epoch 399/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.0490 - val_loss: 4.5285\n",
      "Epoch 400/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0450 - val_loss: 4.5261\n",
      "Epoch 401/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0444 - val_loss: 4.5391\n",
      "Epoch 402/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0412 - val_loss: 4.5393\n",
      "Epoch 403/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0452 - val_loss: 4.5521\n",
      "Epoch 404/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0384 - val_loss: 4.5400\n",
      "Epoch 405/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0370 - val_loss: 4.5438\n",
      "Epoch 406/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0315 - val_loss: 4.5670\n",
      "Epoch 407/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0298 - val_loss: 4.5480\n",
      "Epoch 408/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0264 - val_loss: 4.5684\n",
      "Epoch 409/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 2.0276 - val_loss: 4.5701\n",
      "Epoch 410/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0189 - val_loss: 4.5606\n",
      "Epoch 411/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0169 - val_loss: 4.5776\n",
      "Epoch 412/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0148 - val_loss: 4.5717\n",
      "Epoch 413/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0161 - val_loss: 4.5773\n",
      "Epoch 414/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0139 - val_loss: 4.5834\n",
      "Epoch 415/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0113 - val_loss: 4.5868\n",
      "Epoch 416/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0114 - val_loss: 4.5941\n",
      "Epoch 417/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0046 - val_loss: 4.6089\n",
      "Epoch 418/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 2.0024 - val_loss: 4.6015\n",
      "Epoch 419/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.9931 - val_loss: 4.5891\n",
      "Epoch 420/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.9970 - val_loss: 4.5987\n",
      "Epoch 421/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9958 - val_loss: 4.5953\n",
      "Epoch 422/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9894 - val_loss: 4.6118\n",
      "Epoch 423/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9835 - val_loss: 4.6140\n",
      "Epoch 424/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9812 - val_loss: 4.6079\n",
      "Epoch 425/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9836 - val_loss: 4.6131\n",
      "Epoch 426/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9771 - val_loss: 4.6033\n",
      "Epoch 427/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9726 - val_loss: 4.6133\n",
      "Epoch 428/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9695 - val_loss: 4.6272\n",
      "Epoch 429/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9653 - val_loss: 4.6331\n",
      "Epoch 430/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9627 - val_loss: 4.6256\n",
      "Epoch 431/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9548 - val_loss: 4.6291\n",
      "Epoch 432/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9549 - val_loss: 4.6346\n",
      "Epoch 433/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9508 - val_loss: 4.6383\n",
      "Epoch 434/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9492 - val_loss: 4.6290\n",
      "Epoch 435/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9459 - val_loss: 4.6393\n",
      "Epoch 436/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.9457 - val_loss: 4.6548\n",
      "Epoch 437/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9404 - val_loss: 4.6446\n",
      "Epoch 438/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9362 - val_loss: 4.6588\n",
      "Epoch 439/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9300 - val_loss: 4.6490\n",
      "Epoch 440/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9275 - val_loss: 4.6526\n",
      "Epoch 441/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9257 - val_loss: 4.6476\n",
      "Epoch 442/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9196 - val_loss: 4.6671\n",
      "Epoch 443/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9201 - val_loss: 4.6612\n",
      "Epoch 444/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9131 - val_loss: 4.6685\n",
      "Epoch 445/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.9141 - val_loss: 4.6817\n",
      "Epoch 446/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9183 - val_loss: 4.6775\n",
      "Epoch 447/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9138 - val_loss: 4.6813\n",
      "Epoch 448/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9095 - val_loss: 4.6670\n",
      "Epoch 449/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.9012 - val_loss: 4.6938\n",
      "Epoch 450/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8992 - val_loss: 4.6906\n",
      "Epoch 451/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8961 - val_loss: 4.6969\n",
      "Epoch 452/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8931 - val_loss: 4.7115\n",
      "Epoch 453/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8929 - val_loss: 4.7062\n",
      "Epoch 454/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8893 - val_loss: 4.7103\n",
      "Epoch 455/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8882 - val_loss: 4.7060\n",
      "Epoch 456/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8886 - val_loss: 4.7175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8786 - val_loss: 4.7150\n",
      "Epoch 458/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8764 - val_loss: 4.7290\n",
      "Epoch 459/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8777 - val_loss: 4.7303\n",
      "Epoch 460/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8725 - val_loss: 4.7270\n",
      "Epoch 461/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8662 - val_loss: 4.7264\n",
      "Epoch 462/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8588 - val_loss: 4.7391\n",
      "Epoch 463/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8610 - val_loss: 4.7419\n",
      "Epoch 464/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8581 - val_loss: 4.7483\n",
      "Epoch 465/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8594 - val_loss: 4.7438\n",
      "Epoch 466/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8576 - val_loss: 4.7525\n",
      "Epoch 467/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8513 - val_loss: 4.7596\n",
      "Epoch 468/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8491 - val_loss: 4.7520\n",
      "Epoch 469/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8493 - val_loss: 4.7540\n",
      "Epoch 470/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8404 - val_loss: 4.7660\n",
      "Epoch 471/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8395 - val_loss: 4.7707\n",
      "Epoch 472/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8343 - val_loss: 4.7746\n",
      "Epoch 473/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8356 - val_loss: 4.7694\n",
      "Epoch 474/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8271 - val_loss: 4.7767\n",
      "Epoch 475/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8224 - val_loss: 4.7864\n",
      "Epoch 476/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8297 - val_loss: 4.7840\n",
      "Epoch 477/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8184 - val_loss: 4.7986\n",
      "Epoch 478/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8143 - val_loss: 4.8113\n",
      "Epoch 479/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8132 - val_loss: 4.8067\n",
      "Epoch 480/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8139 - val_loss: 4.8114\n",
      "Epoch 481/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.8096 - val_loss: 4.8038\n",
      "Epoch 482/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8068 - val_loss: 4.8031\n",
      "Epoch 483/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8105 - val_loss: 4.8197\n",
      "Epoch 484/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.8040 - val_loss: 4.8273\n",
      "Epoch 485/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.7951 - val_loss: 4.8327\n",
      "Epoch 486/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7936 - val_loss: 4.8439\n",
      "Epoch 487/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7889 - val_loss: 4.8527\n",
      "Epoch 488/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.7880 - val_loss: 4.8611\n",
      "Epoch 489/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7851 - val_loss: 4.8575\n",
      "Epoch 490/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7829 - val_loss: 4.8504\n",
      "Epoch 491/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7785 - val_loss: 4.8531\n",
      "Epoch 492/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7754 - val_loss: 4.8573\n",
      "Epoch 493/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7699 - val_loss: 4.8718\n",
      "Epoch 494/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.7679 - val_loss: 4.8737\n",
      "Epoch 495/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7647 - val_loss: 4.8751\n",
      "Epoch 496/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7637 - val_loss: 4.8816\n",
      "Epoch 497/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7706 - val_loss: 4.8776\n",
      "Epoch 498/500\n",
      "18201/18201 [==============================] - 74s 4ms/step - loss: 1.7562 - val_loss: 4.9025\n",
      "Epoch 499/500\n",
      "18201/18201 [==============================] - 75s 4ms/step - loss: 1.7571 - val_loss: 4.8995\n",
      "Epoch 500/500\n",
      "18201/18201 [==============================] - 60s 3ms/step - loss: 1.7531 - val_loss: 4.9175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fed6294e9b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anneke/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, 3000)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None, 3000)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 100), (None, 1240400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 100),  1240400     input_2[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 3000)   303000      lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,783,800\n",
      "Trainable params: 2,783,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling models\n",
    "\n",
    "# https://nlp.stanford.edu/~johnhew/public/14-seq2seq.pdf\n",
    "# https://medium.com/machine-learning-bites/deeplearning-series-sequence-to-sequence-architectures-4c4ca89e5654\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                     [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anneke/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'input_5:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'input_6:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# encoder_model.save('encoder.h5')\n",
    "# decoder_model.save('decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict((i,word) for word,i in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "#     target_seq = np.random.rand(1,1, num_decoder_tokens)\n",
    "    target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    \n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    for i in range(5):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "#         print(np.max(output_tokens))\n",
    "#         print(sorted(output_tokens[0, -1, :])[-5:])\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_word = ' '\n",
    "        else:\n",
    "            sampled_word = reverse_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        target_seq[0,0,sampled_token_index] = 1.\n",
    "        \n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_target(input_seq, true_target_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    true_states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    from_true_target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    true_token_index = true_target_seq[0,0]\n",
    "    target_seq[0,0,true_token_index] = 1.\n",
    "    \n",
    "    decoded_sentence = ''\n",
    "    if true_token_index == 0:\n",
    "            sampled_word = ' '\n",
    "    else:\n",
    "            sampled_word = reverse_word_index[true_token_index]\n",
    "            \n",
    "    decoded_sentence += ' ' + sampled_word\n",
    "    \n",
    "    true_joint_log_prob = 0\n",
    "    pred_joint_log_prob = 0\n",
    "    \n",
    "    for i in range(4):\n",
    "        # from prediction\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # from true target\n",
    "        from_true_output_tokens, h_true, c_true = decoder_model.predict([from_true_target_seq] + true_states_value)\n",
    "        \n",
    "        predict_target_prob = np.max(output_tokens[0, -1, :])\n",
    "        true_target_prob = from_true_output_tokens[0,-1, true_target_seq[0,i]]\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        true_token_index = true_target_seq[0,i]\n",
    "        \n",
    "        true_joint_log_prob += np.log(true_target_prob)\n",
    "        pred_joint_log_prob += np.log(predict_target_prob)\n",
    "        \n",
    "        if sampled_token_index == 0:\n",
    "            sampled_word = ' '\n",
    "        else:\n",
    "            sampled_word = reverse_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        target_seq[0,0,sampled_token_index] = 1.\n",
    "        \n",
    "        states_value = [h, c]\n",
    "        \n",
    "        from_true_target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        from_true_target_seq[0,0,true_token_index] = 1.\n",
    "        \n",
    "        true_states_value = [h_true, c_true]\n",
    "    \n",
    "#     print(true_joint_log_prob, pred_joint_log_prob)\n",
    "    \n",
    "    # print the last prob\n",
    "#     print(true_target_prob, predict_target_prob)\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output sentence:  far the best movie ever\n",
      "True Target sentence: far the best show on\n",
      "\n",
      "\n",
      "Output sentence:  a total waste of time\n",
      "True Target sentence: a total waste of time\n",
      "\n",
      "\n",
      "Output sentence:  imagery how beautifully a making\n",
      "True Target sentence: imagery how beautifully the opening\n",
      "\n",
      "\n",
      "Output sentence:    was best described as\n",
      "True Target sentence: borel is unfortunately not a\n",
      "\n",
      "\n",
      "Output sentence:  of the best performance in\n",
      "True Target sentence: of the worst film i\n",
      "\n",
      "\n",
      "Output sentence:    the best known for\n",
      "True Target sentence: the always fantastic kevin spacey\n",
      "\n",
      "\n",
      "Output sentence:    and the excellent story\n",
      "True Target sentence: was so beautiful then\n",
      "\n",
      "\n",
      "Output sentence:  do not waste your time\n",
      "True Target sentence: do not waste your time\n",
      "\n",
      "\n",
      "Output sentence:    and the fantastic of\n",
      "True Target sentence: is my favorite of vincent\n",
      "\n",
      "\n",
      "Output sentence:  during the best of the\n",
      "True Target sentence: during the worst terrorist attack\n",
      "\n",
      "\n",
      "Output sentence:      the best enjoyed\n",
      "True Target sentence: single most annoying and convoluted\n",
      "\n",
      "\n",
      "Output sentence:  shallow and forgettable cop movie\n",
      "True Target sentence: shallow and forgettable cop movie\n",
      "\n",
      "\n",
      "Output sentence:  again i waste of the\n",
      "True Target sentence: again another disappointing movie that\n",
      "\n",
      "\n",
      "Output sentence:  that is boring and not\n",
      "True Target sentence: that works perfectly for de\n",
      "\n",
      "\n",
      "Output sentence:      unfortunately the film\n",
      "True Target sentence: joan crawford's best talky it\n",
      "\n",
      "\n",
      "Output sentence:    a poor attempt at\n",
      "True Target sentence: a 100 best list hustle\n",
      "\n",
      "\n",
      "Output sentence:  is an excellent and my\n",
      "True Target sentence: is a terrible movie terrible\n",
      "\n",
      "\n",
      "Output sentence:  was just awful the effect\n",
      "True Target sentence: was pretty disappointed as has\n",
      "\n",
      "\n",
      "Output sentence:  the single worst battle for\n",
      "True Target sentence: the single worst show on\n",
      "\n",
      "\n",
      "Output sentence:  life 2 10 i would\n",
      "True Target sentence: your life 2/10 i am\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(80,100):\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    true_target_seq = y_tr_padded[seq_index:seq_index+1]\n",
    "    decoded_sentence = decode_sequence_target(input_seq, true_target_seq)\n",
    "    \n",
    "#     print('Input sentence:', X_train_sequence[seq_index])\n",
    "    print()\n",
    "    print('Output sentence:', decoded_sentence)\n",
    "    print('True Target sentence:', y_train_target[seq_index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play with Candidate Sequence\n",
    "\n",
    "### Without the predicted prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with candidate\n",
    "\n",
    "def decode_sequence_target(input_seq, candidate_target_seq):\n",
    "    candidate_states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    from_candidate_target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    candidate_token_index = candidate_target_seq[0,0]\n",
    "    from_candidate_target_seq[0,0,candidate_token_index] = 1.\n",
    "    candidate_joint_log_prob = 0\n",
    "    \n",
    "    for i in range(4):\n",
    "        from_candidate_output_tokens, h_true, c_true = decoder_model.predict([from_candidate_target_seq] + candidate_states_value)\n",
    "    \n",
    "        candidate_target_prob = from_candidate_output_tokens[0,-1, candidate_target_seq[0,i]]\n",
    "        candidate_token_index = candidate_target_seq[0,i]\n",
    "        candidate_joint_log_prob += np.log(candidate_target_prob)\n",
    "        \n",
    "        # get the t+1 input\n",
    "        from_candidate_target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        from_candidate_target_seq[0,0,candidate_token_index] = 1.\n",
    "        \n",
    "        candidate_states_value = [h_true, c_true]\n",
    "\n",
    "    return candidate_joint_log_prob, candidate_target_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: even kind of a happy ending of sort whee a step up from part 4 but not much of one again brian yuzna is involved and screaming mad george so some decent special effect but not enough to make this great a few leftover from part 4 are hanging around too like clint howard and neith hunter but that does not really make any difference anyway i now have seeing the whole series out of my system now if i could \n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[0 0 0 0 0]]\n",
      "1\t-98.5723982 \t 0.0000072\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[0 0 0 0 0]]\n",
      "2\t-98.5723982 \t 0.0000072\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[0 0 0 0 0]]\n",
      "3\t-98.5723982 \t 0.0000072\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[0 0 0 0 0]]\n",
      "4\t-98.5723982 \t 0.0000072\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[0 0 0 0 0]]\n",
      "5\t-98.5723982 \t 0.0000072\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 0  0  0  0 60]]\n",
      "6\t-98.5723982 \t 0.0000072\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  0   0   0  60 241]]\n",
      "7\t-93.1368947 \t 0.0016627\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  0   0  60 241   5]]\n",
      "8\t-77.1534371 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  0  60 241   5   3]]\n",
      "9\t-69.9408255 \t 0.0018453\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 60 241   5   3 716]]\n",
      "10\t-32.6750093 \t 0.0046297\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[241   5   3 716 258]]\n",
      "11\t-36.7305245 \t 0.0000022\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  5   3 716 258   5]]\n",
      "12\t-67.8308992 \t 0.0000021\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  3 716 258   5 439]]\n",
      "13\t-68.3757401 \t 0.0000055\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[716 258   5 439   3]]\n",
      "14\t-76.3506813 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 258    5  439    3 1087]]\n",
      "15\t-72.3425603 \t 0.0107069\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[   5  439    3 1087   65]]\n",
      "16\t-96.3699856 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 439    3 1087   65   36]]\n",
      "17\t-88.5043621 \t 0.0000042\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[   3 1087   65   36  129]]\n",
      "18\t-88.5269852 \t 0.0000224\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[1087   65   36  129  408]]\n",
      "19\t-90.0790262 \t 0.0000007\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 65  36 129 408  17]]\n",
      "20\t-55.3765678 \t 0.0000624\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 36 129 408  17  12]]\n",
      "21\t-50.6215124 \t 0.0001647\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[129 408  17  12  79]]\n",
      "22\t-35.5174336 \t 0.0243947\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[408  17  12  79   5]]\n",
      "23\t-44.1513028 \t 0.0008635\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[17 12 79  5 24]]\n",
      "24\t-39.0339270 \t 0.0000610\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 12  79   5  24 181]]\n",
      "25\t-43.2369432 \t 0.0000004\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  79    5   24  181 1544]]\n",
      "26\t-42.2820952 \t 0.0000683\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[   5   24  181 1544    4]]\n",
      "27\t-71.3715611 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  24  181 1544    4  625]]\n",
      "28\t-49.7375245 \t 0.0010279\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 181 1544    4  625    2]]\n",
      "29\t-98.2433643 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[1544    4  625    2 1720]]\n",
      "30\t-106.5141439 \t 0.0003169\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[   4  625    2 1720 1253]]\n",
      "31\t-146.9805632 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 625    2 1720 1253  695]]\n",
      "32\t-162.7727327 \t 0.0001983\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[   2 1720 1253  695   30]]\n",
      "33\t-103.5669394 \t 0.0000001\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[1720 1253  695   30   46]]\n",
      "34\t-106.7082005 \t 0.0000006\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[1253  695   30   46  479]]\n",
      "35\t-60.7490935 \t 0.0034752\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[695  30  46 479 264]]\n",
      "36\t-51.9802752 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 30  46 479 264 200]]\n",
      "37\t-49.5227616 \t 0.0000386\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 46 479 264 200  17]]\n",
      "38\t-38.3783914 \t 0.9440195\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[479 264 200  17  12]]\n",
      "39\t-31.7621974 \t 0.0130911\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[264 200  17  12 208]]\n",
      "40\t-21.2938309 \t 0.0204523\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[200  17  12 208   6]]\n",
      "41\t-37.3420086 \t 0.0156262\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 17  12 208   6  98]]\n",
      "42\t-25.9206967 \t 0.5700696\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 12 208   6  98  10]]\n",
      "43\t-26.4017558 \t 0.0005284\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[208   6  98  10  56]]\n",
      "44\t-33.7410259 \t 0.0015885\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 6 98 10 56  3]]\n",
      "45\t-37.8258762 \t 0.0000060\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 98  10  56   3 167]]\n",
      "46\t-49.1256399 \t 0.0003505\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 10  56   3 167  36]]\n",
      "47\t-20.4426081 \t 0.1020510\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 56   3 167  36 129]]\n",
      "48\t-51.0747638 \t 0.0006105\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  3 167  36 129 408]]\n",
      "49\t-50.2349138 \t 0.0000032\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[167  36 129 408  21]]\n",
      "50\t-55.2983313 \t 0.0000060\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  36  129  408   21 2757]]\n",
      "51\t-46.1265531 \t 0.0147516\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 129  408   21 2757  227]]\n",
      "52\t-46.4852252 \t 0.0000001\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 408   21 2757  227  101]]\n",
      "53\t-63.9583044 \t 0.0000001\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  21 2757  227  101   32]]\n",
      "54\t-68.0851107 \t 0.0007283\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[2757  227  101   32 2188]]\n",
      "55\t-51.1989856 \t 0.0001641\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 227  101   32 2188    2]]\n",
      "56\t-95.9324999 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 101   32 2188    2 2189]]\n",
      "57\t-88.7336862 \t 0.0021343\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  32 2188    2 2189   17]]\n",
      "58\t-103.8745365 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[2188    2 2189   17   11]]\n",
      "59\t-74.9645100 \t 0.0000028\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[   2 2189   17   11   82]]\n",
      "60\t-54.0771379 \t 0.0097344\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[2189   17   11   82   12]]\n",
      "61\t-57.7507405 \t 0.0015921\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[17 11 82 12 61]]\n",
      "62\t-47.5558262 \t 0.0001451\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[11 82 12 61 98]]\n",
      "63\t-38.8418238 \t 0.0079650\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 82  12  61  98 108]]\n",
      "64\t-37.7406502 \t 0.0001192\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  12   61   98  108 1440]]\n",
      "65\t-36.0506487 \t 0.0021416\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  61   98  108 1440  515]]\n",
      "66\t-47.1848845 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  98  108 1440  515    8]]\n",
      "67\t-64.8882761 \t 0.0001882\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 108 1440  515    8  160]]\n",
      "68\t-49.4442711 \t 0.0011005\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[1440  515    8  160   22]]\n",
      "69\t-64.9982622 \t 0.0000005\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[515   8 160  22 301]]\n",
      "70\t-51.4257565 \t 0.0000253\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  8 160  22 301   1]]\n",
      "71\t-57.3720045 \t 0.0000095\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[160  22 301   1 202]]\n",
      "72\t-49.2207646 \t 0.0046870\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 22 301   1 202 184]]\n",
      "73\t-46.5805163 \t 0.0009917\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[301   1 202 184  50]]\n",
      "74\t-59.6054306 \t 0.0000000\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  1 202 184  50   5]]\n",
      "75\t-43.2897844 \t 0.0000493\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[202 184  50   5  57]]\n",
      "76\t-39.0628848 \t 0.0021762\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[ 184   50    5   57 1501]]\n",
      "77\t-41.6054320 \t 0.0003778\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[  50    5   57 1501  160]]\n",
      "78\t-51.3717451 \t 0.0019707\n",
      "\n",
      "true sequence: [98, 10, 56, 3, 167]\n",
      "Candidate sequence: [[   5   57 1501  160   43]]\n",
      "79\t-61.6393394 \t 0.0000009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq_index = 0\n",
    "\n",
    "candidate_jll = []\n",
    "sliding_y_tr = []\n",
    "\n",
    "for i in range(X_tr_padded[seq_index].shape[0]-1-5):\n",
    "    sliding_y_tr.append(X_tr_padded[0, i:i+5])\n",
    "\n",
    "sliding_y_tr = np.asarray(sliding_y_tr)\n",
    "\n",
    "print('Input sentence:', X_train_sequence[seq_index], '\\n')\n",
    "\n",
    "for i in range(sliding_y_tr.shape[0]):\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    candidate_seq = sliding_y_tr[i:i+1]\n",
    "    candidate_jll_slide, candidate_last_prob = decode_sequence_target(input_seq, candidate_seq)\n",
    "    \n",
    "    candidate_jll.append([candidate_jll_slide, candidate_last_prob])\n",
    "    print('true sequence:', y_tr_sequence[0])\n",
    "    print('Candidate sequence:', candidate_seq)\n",
    "    print('%d\\t%.7f \\t %.7f' %(i+1, candidate_jll_slide, candidate_last_prob))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '          ',\n",
       " '        even ',\n",
       " '      even kind ',\n",
       " '    even kind of ',\n",
       " '  even kind of a ',\n",
       " 'even kind of a happy ',\n",
       " 'kind of a happy ending ',\n",
       " 'of a happy ending of ',\n",
       " 'a happy ending of sort ',\n",
       " 'happy ending of sort a ',\n",
       " 'ending of sort a step ',\n",
       " 'of sort a step up ',\n",
       " 'sort a step up from ',\n",
       " 'a step up from part ',\n",
       " 'step up from part 4 ',\n",
       " 'up from part 4 but ',\n",
       " 'from part 4 but not ',\n",
       " 'part 4 but not much ',\n",
       " '4 but not much of ',\n",
       " 'but not much of one ',\n",
       " 'not much of one again ',\n",
       " 'much of one again brian ',\n",
       " 'of one again brian is ',\n",
       " 'one again brian is involved ',\n",
       " 'again brian is involved and ',\n",
       " 'brian is involved and screaming ',\n",
       " 'is involved and screaming mad ',\n",
       " 'involved and screaming mad george ',\n",
       " 'and screaming mad george so ',\n",
       " 'screaming mad george so some ',\n",
       " 'mad george so some decent ',\n",
       " 'george so some decent special ',\n",
       " 'so some decent special effect ',\n",
       " 'some decent special effect but ',\n",
       " 'decent special effect but not ',\n",
       " 'special effect but not enough ',\n",
       " 'effect but not enough to ',\n",
       " 'but not enough to make ',\n",
       " 'not enough to make this ',\n",
       " 'enough to make this great ',\n",
       " 'to make this great a ',\n",
       " 'make this great a few ',\n",
       " 'this great a few from ',\n",
       " 'great a few from part ',\n",
       " 'a few from part 4 ',\n",
       " 'few from part 4 are ',\n",
       " 'from part 4 are hanging ',\n",
       " 'part 4 are hanging around ',\n",
       " '4 are hanging around too ',\n",
       " 'are hanging around too like ',\n",
       " 'hanging around too like howard ',\n",
       " 'around too like howard and ',\n",
       " 'too like howard and hunter ',\n",
       " 'like howard and hunter but ',\n",
       " 'howard and hunter but that ',\n",
       " 'and hunter but that does ',\n",
       " 'hunter but that does not ',\n",
       " 'but that does not really ',\n",
       " 'that does not really make ',\n",
       " 'does not really make any ',\n",
       " 'not really make any difference ',\n",
       " 'really make any difference anyway ',\n",
       " 'make any difference anyway i ',\n",
       " 'any difference anyway i now ',\n",
       " 'difference anyway i now have ',\n",
       " 'anyway i now have seeing ',\n",
       " 'i now have seeing the ',\n",
       " 'now have seeing the whole ',\n",
       " 'have seeing the whole series ',\n",
       " 'seeing the whole series out ',\n",
       " 'the whole series out of ',\n",
       " 'whole series out of my ',\n",
       " 'series out of my system ',\n",
       " 'out of my system now ',\n",
       " 'of my system now if ']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_slide = []\n",
    "for i in sliding_y_tr:\n",
    "    decode_sentence = ''\n",
    "    for j in i:\n",
    "        if j == 0:\n",
    "            word = ' '\n",
    "        else:\n",
    "            word = reverse_word_index[j]\n",
    "        decode_sentence += word + ' '\n",
    "    decode_slide.append(decode_sentence)\n",
    "    \n",
    "decode_slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this great a few from'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_slide[46]\n",
    "\n",
    "# we can calculate how far it is from the target sequence in terms of index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-20.44260811805725, 46)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_jll = np.asarray(candidate_jll)\n",
    "\n",
    "np.max(candidate_jll[:,0]), np.argmax(candidate_jll[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  56,   3, 167,  36], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_y_tr[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,    0],\n",
       "       [   0,    0,    0,    0,   60],\n",
       "       [   0,    0,    0,   60,  241],\n",
       "       [   0,    0,   60,  241,    5],\n",
       "       [   0,   60,  241,    5,    3],\n",
       "       [  60,  241,    5,    3,  716],\n",
       "       [ 241,    5,    3,  716,  258],\n",
       "       [   5,    3,  716,  258,    5],\n",
       "       [   3,  716,  258,    5,  439],\n",
       "       [ 716,  258,    5,  439,    3],\n",
       "       [ 258,    5,  439,    3, 1087],\n",
       "       [   5,  439,    3, 1087,   65],\n",
       "       [ 439,    3, 1087,   65,   36],\n",
       "       [   3, 1087,   65,   36,  129],\n",
       "       [1087,   65,   36,  129,  408],\n",
       "       [  65,   36,  129,  408,   17],\n",
       "       [  36,  129,  408,   17,   12],\n",
       "       [ 129,  408,   17,   12,   79],\n",
       "       [ 408,   17,   12,   79,    5],\n",
       "       [  17,   12,   79,    5,   24],\n",
       "       [  12,   79,    5,   24,  181],\n",
       "       [  79,    5,   24,  181, 1544],\n",
       "       [   5,   24,  181, 1544,    4],\n",
       "       [  24,  181, 1544,    4,  625],\n",
       "       [ 181, 1544,    4,  625,    2],\n",
       "       [1544,    4,  625,    2, 1720],\n",
       "       [   4,  625,    2, 1720, 1253],\n",
       "       [ 625,    2, 1720, 1253,  695],\n",
       "       [   2, 1720, 1253,  695,   30],\n",
       "       [1720, 1253,  695,   30,   46],\n",
       "       [1253,  695,   30,   46,  479],\n",
       "       [ 695,   30,   46,  479,  264],\n",
       "       [  30,   46,  479,  264,  200],\n",
       "       [  46,  479,  264,  200,   17],\n",
       "       [ 479,  264,  200,   17,   12],\n",
       "       [ 264,  200,   17,   12,  208],\n",
       "       [ 200,   17,   12,  208,    6],\n",
       "       [  17,   12,  208,    6,   98],\n",
       "       [  12,  208,    6,   98,   10],\n",
       "       [ 208,    6,   98,   10,   56],\n",
       "       [   6,   98,   10,   56,    3],\n",
       "       [  98,   10,   56,    3,  167],\n",
       "       [  10,   56,    3,  167,   36],\n",
       "       [  56,    3,  167,   36,  129],\n",
       "       [   3,  167,   36,  129,  408],\n",
       "       [ 167,   36,  129,  408,   21],\n",
       "       [  36,  129,  408,   21, 2757],\n",
       "       [ 129,  408,   21, 2757,  227],\n",
       "       [ 408,   21, 2757,  227,  101],\n",
       "       [  21, 2757,  227,  101,   32],\n",
       "       [2757,  227,  101,   32, 2188],\n",
       "       [ 227,  101,   32, 2188,    2],\n",
       "       [ 101,   32, 2188,    2, 2189],\n",
       "       [  32, 2188,    2, 2189,   17],\n",
       "       [2188,    2, 2189,   17,   11],\n",
       "       [   2, 2189,   17,   11,   82],\n",
       "       [2189,   17,   11,   82,   12],\n",
       "       [  17,   11,   82,   12,   61],\n",
       "       [  11,   82,   12,   61,   98],\n",
       "       [  82,   12,   61,   98,  108],\n",
       "       [  12,   61,   98,  108, 1440],\n",
       "       [  61,   98,  108, 1440,  515],\n",
       "       [  98,  108, 1440,  515,    8],\n",
       "       [ 108, 1440,  515,    8,  160],\n",
       "       [1440,  515,    8,  160,   22],\n",
       "       [ 515,    8,  160,   22,  301],\n",
       "       [   8,  160,   22,  301,    1],\n",
       "       [ 160,   22,  301,    1,  202],\n",
       "       [  22,  301,    1,  202,  184],\n",
       "       [ 301,    1,  202,  184,   50],\n",
       "       [   1,  202,  184,   50,    5],\n",
       "       [ 202,  184,   50,    5,   57],\n",
       "       [ 184,   50,    5,   57, 1501],\n",
       "       [  50,    5,   57, 1501,  160],\n",
       "       [   5,   57, 1501,  160,   43]], dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_y_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_list(X):\n",
    "    y_candidate = []\n",
    "    \n",
    "    for i in range(X.shape[0]-1-5):\n",
    "        y_candidate.append(X[i:i+5])\n",
    "    \n",
    "    return np.asarray(y_candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"jll.csv\", \"a\")\n",
    "start = 2198\n",
    "i = 2198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 2200...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anneke/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 2300...\n",
      "Processing document 2400...\n",
      "Processing document 2500...\n",
      "Processing document 2600...\n",
      "Processing document 2700...\n",
      "Processing document 2800...\n",
      "Processing document 2900...\n",
      "Processing document 3000...\n",
      "Processing document 3100...\n",
      "Processing document 3200...\n",
      "Processing document 3300...\n",
      "Processing document 3400...\n",
      "Processing document 3500...\n",
      "Processing document 3600...\n",
      "Processing document 3700...\n",
      "Processing document 3800...\n",
      "Processing document 3900...\n",
      "Processing document 4000...\n",
      "Processing document 4100...\n",
      "Processing document 4200...\n",
      "Processing document 4300...\n",
      "Processing document 4400...\n",
      "Processing document 4500...\n",
      "Processing document 4600...\n",
      "Processing document 4700...\n",
      "Processing document 4800...\n",
      "Processing document 4900...\n",
      "Processing document 5000...\n",
      "Processing document 5100...\n",
      "Processing document 5200...\n",
      "Processing document 5300...\n",
      "Processing document 5400...\n",
      "Processing document 5500...\n",
      "Processing document 5600...\n",
      "Processing document 5700...\n",
      "Processing document 5800...\n",
      "Processing document 5900...\n",
      "Processing document 6000...\n",
      "Processing document 6100...\n",
      "Processing document 6200...\n",
      "Processing document 6300...\n",
      "Processing document 6400...\n",
      "Processing document 6500...\n",
      "Processing document 6600...\n",
      "Processing document 6700...\n",
      "Processing document 6800...\n",
      "Processing document 6900...\n",
      "Processing document 7000...\n",
      "Processing document 7100...\n",
      "Processing document 7200...\n",
      "Processing document 7300...\n",
      "Processing document 7400...\n",
      "Processing document 7500...\n",
      "Processing document 7600...\n",
      "Processing document 7700...\n",
      "Processing document 7800...\n",
      "Processing document 7900...\n",
      "Processing document 8000...\n",
      "Processing document 8100...\n",
      "Processing document 8200...\n",
      "Processing document 8300...\n",
      "Processing document 8400...\n",
      "Processing document 8500...\n",
      "Processing document 8600...\n",
      "Processing document 8700...\n",
      "Processing document 8800...\n",
      "Processing document 8900...\n",
      "Processing document 9000...\n",
      "Processing document 9100...\n",
      "Processing document 9200...\n",
      "Processing document 9300...\n",
      "Processing document 9400...\n",
      "Processing document 9500...\n",
      "Processing document 9600...\n",
      "Processing document 9700...\n",
      "Processing document 9800...\n",
      "Processing document 9900...\n",
      "Processing document 10000...\n",
      "Processing document 10100...\n",
      "Processing document 10200...\n",
      "Processing document 10300...\n",
      "Processing document 10400...\n",
      "Processing document 10500...\n",
      "Processing document 10600...\n",
      "Processing document 10700...\n",
      "Processing document 10800...\n",
      "Processing document 10900...\n",
      "Processing document 11000...\n",
      "Processing document 11100...\n",
      "Processing document 11200...\n",
      "Processing document 11300...\n",
      "Processing document 11400...\n",
      "Processing document 11500...\n",
      "Processing document 11600...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-d0aba45f778e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcandidate_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_candidate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcandidate_jll_slide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_last_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_states_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mcandidate_jll_per_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_jll_slide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-ddaf1e89a469>\u001b[0m in \u001b[0;36mdecode_sequence_target\u001b[0;34m(candidate_states_value, candidate_target_seq)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mfrom_candidate_output_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrom_candidate_target_seq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcandidate_states_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcandidate_target_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_candidate_output_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_target_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for doc in X_tr_padded[start:]:\n",
    "    y_candidate = generate_candidate_list(doc)\n",
    "    \n",
    "    candidate_jll_per_doc = []\n",
    "    input_seq = encoder_input_data[i:i+1]\n",
    "    \n",
    "    true_target_index = target_index(i, y_candidate, y_tr_sequence[i])\n",
    "    \n",
    "    # Encode\n",
    "    candidate_states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    for j in range(y_candidate.shape[0]):\n",
    "        candidate_seq = y_candidate[j:j+1]\n",
    "        candidate_jll_slide, candidate_last_prob = decode_sequence_target(candidate_states_value, candidate_seq)\n",
    "        candidate_jll_per_doc.append(candidate_jll_slide)\n",
    "\n",
    "    candidate_jll_per_doc = np.asarray(candidate_jll_per_doc)\n",
    "    max_jll_index = np.argmax(candidate_jll_per_doc)\n",
    "    true_target_jll = np.around(candidate_jll_per_doc[true_target_index],5)\n",
    "    max_candidate_jll = np.around(candidate_jll_per_doc[max_jll_index],5)\n",
    "    \n",
    "    \n",
    "    file.write('%d\\t%d\\t%s\\t%d\\t%s\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%d\\n' %(i, true_target_index, y_train_target[i],\n",
    "                                                            max_jll_index, to_sequence(y_candidate[max_jll_index]),\n",
    "                                                            -(true_target_index-max_jll_index),\n",
    "                                                            true_target_jll, max_candidate_jll,\n",
    "                                                            np.absolute(true_target_jll-max_candidate_jll),\n",
    "                                                            len(intersection(y_tr_sequence[i], y_candidate[max_jll_index]))))\n",
    "    if i % 100 == 0:\n",
    "        print('Processing document %d...' %(i))\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2198"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc num, doc index argmax\n",
    "\n",
    "def to_sequence(int_sequence):\n",
    "    decoded = ''\n",
    "    for i in int_sequence:\n",
    "        if i == 0:\n",
    "            word = ' '\n",
    "        else:\n",
    "            word = reverse_word_index[i]\n",
    "        decoded += word + ' '\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with candidate\n",
    "\n",
    "def decode_sequence_target(candidate_states_value, candidate_target_seq):\n",
    "#     candidate_states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    from_candidate_target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    candidate_token_index = candidate_target_seq[0,0]\n",
    "    from_candidate_target_seq[0,0,candidate_token_index] = 1.\n",
    "    candidate_joint_log_prob = 0\n",
    "    \n",
    "    for i in range(4):\n",
    "        from_candidate_output_tokens, h_true, c_true = decoder_model.predict([from_candidate_target_seq] + candidate_states_value)\n",
    "    \n",
    "        candidate_target_prob = from_candidate_output_tokens[0,-1, candidate_target_seq[0,i]]\n",
    "        candidate_token_index = candidate_target_seq[0,i]\n",
    "        candidate_joint_log_prob += np.log(candidate_target_prob)\n",
    "        \n",
    "        # get the t+1 input\n",
    "        from_candidate_target_seq = np.zeros((1,1,num_decoder_tokens))\n",
    "        from_candidate_target_seq[0,0,candidate_token_index] = 1.\n",
    "        \n",
    "        candidate_states_value = [h_true, c_true]\n",
    "\n",
    "    return candidate_joint_log_prob, candidate_target_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 1, 35, 5, 10]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_sequence[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "\n",
    "def target_index(doc_idx, candidate_seq, y):\n",
    "    for i,j in enumerate(candidate_seq):\n",
    "        if len(intersection(j, y)) == len(y):\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(y_candidate):\n",
    "    if len(intersection(j, y_tr_sequence[0])) == len(y_tr_sequence[0]):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
