{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.zeros([10000, 10])\n",
    "for i in range(data.shape[0]):\n",
    "    data[i,:] = np.random.randint(0,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 5., 4., 8., 9., 2., 9., 8., 2.],\n",
       "       [0., 2., 0., 0., 0., 5., 3., 0., 8., 1.],\n",
       "       [5., 6., 7., 4., 8., 3., 5., 6., 8., 5.],\n",
       "       [1., 1., 2., 7., 0., 8., 6., 6., 4., 9.],\n",
       "       [0., 2., 8., 0., 1., 7., 6., 2., 0., 3.],\n",
       "       [1., 9., 5., 0., 0., 7., 0., 9., 4., 2.],\n",
       "       [8., 7., 1., 1., 1., 2., 3., 4., 6., 3.],\n",
       "       [7., 2., 6., 8., 1., 3., 8., 7., 8., 7.],\n",
       "       [0., 2., 2., 6., 2., 8., 2., 8., 6., 9.],\n",
       "       [9., 2., 9., 4., 3., 2., 2., 6., 6., 9.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = np.zeros([data.shape[0], 4])\n",
    "for i in range(len(label)):\n",
    "    pointer = np.random.randint(0,9)\n",
    "    \n",
    "    if pointer > data.shape[1]-5:\n",
    "        # get the last sequence of the data\n",
    "        point = data[i, pointer:]\n",
    "    else:\n",
    "        label[i,:] = data[i, pointer:pointer+4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [3., 5., 6., 8.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0.],\n",
       "       [6., 0., 7., 6.],\n",
       "       [6., 3., 2., 6.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OUTPUTS = label.shape[1]\n",
    "N_INPUTS = data.shape[1]\n",
    "N_HIDDEN_UNITS = 10 # Define here\n",
    "N_EPOCHS = 100 # define here\n",
    "# N_FEATURES = 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, shape=[None, N_INPUTS], name='input')\n",
    "outputs = tf.placeholder(tf.float32, shape=[None, N_OUTPUTS], name='output') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = {\n",
    "    'hidden': tf.Variable(tf.random_normal([N_INPUTS, N_HIDDEN_UNITS])),\n",
    "    'output': tf.Variable(tf.random_normal([N_HIDDEN_UNITS, N_OUTPUTS]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([N_HIDDEN_UNITS], mean=1.0)),\n",
    "    'output': tf.Variable(tf.random_normal([N_OUTPUTS], mean=1.0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 100 loss: -0.2997018\n",
      "Epoch 1 out of 100 loss: -0.55549276\n",
      "Epoch 2 out of 100 loss: -0.81034744\n",
      "Epoch 3 out of 100 loss: -1.0643262\n",
      "Epoch 4 out of 100 loss: -1.3174007\n",
      "Epoch 5 out of 100 loss: -1.5695033\n",
      "Epoch 6 out of 100 loss: -1.8206918\n",
      "Epoch 7 out of 100 loss: -2.070941\n",
      "Epoch 8 out of 100 loss: -2.3201792\n",
      "Epoch 9 out of 100 loss: -2.5684605\n",
      "Epoch 10 out of 100 loss: -2.8158617\n",
      "Epoch 11 out of 100 loss: -3.0623515\n",
      "Epoch 12 out of 100 loss: -3.3079016\n",
      "Epoch 13 out of 100 loss: -3.552503\n",
      "Epoch 14 out of 100 loss: -3.7961652\n",
      "Epoch 15 out of 100 loss: -4.038896\n",
      "Epoch 16 out of 100 loss: -4.280904\n",
      "Epoch 17 out of 100 loss: -4.5220513\n",
      "Epoch 18 out of 100 loss: -4.7622867\n",
      "Epoch 19 out of 100 loss: -5.001736\n",
      "Epoch 20 out of 100 loss: -5.240451\n",
      "Epoch 21 out of 100 loss: -5.4784093\n",
      "Epoch 22 out of 100 loss: -5.715571\n",
      "Epoch 23 out of 100 loss: -5.951968\n",
      "Epoch 24 out of 100 loss: -6.1877384\n",
      "Epoch 25 out of 100 loss: -6.42295\n",
      "Epoch 26 out of 100 loss: -6.6576123\n",
      "Epoch 27 out of 100 loss: -6.8917437\n",
      "Epoch 28 out of 100 loss: -7.125335\n",
      "Epoch 29 out of 100 loss: -7.3584476\n",
      "Epoch 30 out of 100 loss: -7.5913014\n",
      "Epoch 31 out of 100 loss: -7.823746\n",
      "Epoch 32 out of 100 loss: -8.055898\n",
      "Epoch 33 out of 100 loss: -8.287902\n",
      "Epoch 34 out of 100 loss: -8.519858\n",
      "Epoch 35 out of 100 loss: -8.751725\n",
      "Epoch 36 out of 100 loss: -8.983548\n",
      "Epoch 37 out of 100 loss: -9.215286\n",
      "Epoch 38 out of 100 loss: -9.447036\n",
      "Epoch 39 out of 100 loss: -9.678787\n",
      "Epoch 40 out of 100 loss: -9.910784\n",
      "Epoch 41 out of 100 loss: -10.1431875\n",
      "Epoch 42 out of 100 loss: -10.375996\n",
      "Epoch 43 out of 100 loss: -10.6093645\n",
      "Epoch 44 out of 100 loss: -10.843235\n",
      "Epoch 45 out of 100 loss: -11.077634\n",
      "Epoch 46 out of 100 loss: -11.3126\n",
      "Epoch 47 out of 100 loss: -11.54815\n",
      "Epoch 48 out of 100 loss: -11.784229\n",
      "Epoch 49 out of 100 loss: -12.021093\n",
      "Epoch 50 out of 100 loss: -12.258808\n",
      "Epoch 51 out of 100 loss: -12.497319\n",
      "Epoch 52 out of 100 loss: -12.736559\n",
      "Epoch 53 out of 100 loss: -12.976678\n",
      "Epoch 54 out of 100 loss: -13.217764\n",
      "Epoch 55 out of 100 loss: -13.460072\n",
      "Epoch 56 out of 100 loss: -13.703637\n",
      "Epoch 57 out of 100 loss: -13.948491\n",
      "Epoch 58 out of 100 loss: -14.194684\n",
      "Epoch 59 out of 100 loss: -14.442069\n",
      "Epoch 60 out of 100 loss: -14.690868\n",
      "Epoch 61 out of 100 loss: -14.941063\n",
      "Epoch 62 out of 100 loss: -15.192853\n",
      "Epoch 63 out of 100 loss: -15.44627\n",
      "Epoch 64 out of 100 loss: -15.701226\n",
      "Epoch 65 out of 100 loss: -15.9578705\n",
      "Epoch 66 out of 100 loss: -16.216272\n",
      "Epoch 67 out of 100 loss: -16.476429\n",
      "Epoch 68 out of 100 loss: -16.738403\n",
      "Epoch 69 out of 100 loss: -17.002178\n",
      "Epoch 70 out of 100 loss: -17.267897\n",
      "Epoch 71 out of 100 loss: -17.535646\n",
      "Epoch 72 out of 100 loss: -17.805391\n",
      "Epoch 73 out of 100 loss: -18.077091\n",
      "Epoch 74 out of 100 loss: -18.350903\n",
      "Epoch 75 out of 100 loss: -18.626955\n",
      "Epoch 76 out of 100 loss: -18.905142\n",
      "Epoch 77 out of 100 loss: -19.185394\n",
      "Epoch 78 out of 100 loss: -19.467686\n",
      "Epoch 79 out of 100 loss: -19.752155\n",
      "Epoch 80 out of 100 loss: -20.03888\n",
      "Epoch 81 out of 100 loss: -20.328007\n",
      "Epoch 82 out of 100 loss: -20.619616\n",
      "Epoch 83 out of 100 loss: -20.913475\n",
      "Epoch 84 out of 100 loss: -21.209682\n",
      "Epoch 85 out of 100 loss: -21.508276\n",
      "Epoch 86 out of 100 loss: -21.809532\n",
      "Epoch 87 out of 100 loss: -22.11336\n",
      "Epoch 88 out of 100 loss: -22.419857\n",
      "Epoch 89 out of 100 loss: -22.728874\n",
      "Epoch 90 out of 100 loss: -23.040413\n",
      "Epoch 91 out of 100 loss: -23.35431\n",
      "Epoch 92 out of 100 loss: -23.670816\n",
      "Epoch 93 out of 100 loss: -23.989979\n",
      "Epoch 94 out of 100 loss: -24.312002\n",
      "Epoch 95 out of 100 loss: -24.636837\n",
      "Epoch 96 out of 100 loss: -24.96435\n",
      "Epoch 97 out of 100 loss: -25.294659\n",
      "Epoch 98 out of 100 loss: -25.628069\n",
      "Epoch 99 out of 100 loss: -25.964615\n",
      "Accuracy 0.12632499635219574\n",
      "[[9.9999976e-01 1.0000000e+00 1.6284986e-17 1.0000000e+00]\n",
      " [9.9987745e-01 9.9984419e-01 1.3803494e-01 5.7316143e-02]\n",
      " [9.9999690e-01 1.0000000e+00 1.1060570e-02 1.0000000e+00]\n",
      " ...\n",
      " [9.9999976e-01 1.0000000e+00 9.9999988e-01 1.4528659e-05]\n",
      " [1.0000000e+00 1.0000000e+00 1.0000000e+00 2.7571896e-03]\n",
      " [9.9999964e-01 1.0000000e+00 6.6264725e-01 3.3710968e-02]]\n"
     ]
    }
   ],
   "source": [
    "hidden = tf.matmul(inputs, W['hidden']) + biases['hidden']  # hidden layer\n",
    "hidden = tf.nn.relu(hidden)\n",
    "output_ = tf.matmul(hidden, W['output']) + biases['output']  # outputs\n",
    "# split_layer = tf.split(output_,num_or_size_splits=N_OUTPUTS,axis=1)\n",
    "preds = tf.nn.sigmoid(output_)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output_,\n",
    "                                                             labels=outputs))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "correct_preds = tf.equal(tf.cast(tf.greater(preds, tf.constant(0.5)), tf.float32), \n",
    "                         outputs)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "                                  \n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        _, loss_per_epochs = session.run([optimizer, loss],feed_dict={inputs:data, outputs:label}) #should feed inputs and outputs as [Ax,Ay,Az]\n",
    "        print('Epoch', epoch, 'out of', N_EPOCHS, 'loss:', loss_per_epochs)\n",
    "    \n",
    "    accuracy_test = session.run(accuracy, \n",
    "                             feed_dict={inputs:data, outputs:label})\n",
    "    \n",
    "    print('Accuracy {0}'.format(accuracy_test))\n",
    "    \n",
    "    preds = session.run(preds, feed_dict={inputs:data})\n",
    "    \n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 8.5393603e-33],\n",
       "       [9.9999964e-01, 1.0000000e+00, 1.1256049e-07, 1.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 3.4758610e-37, 3.0898945e-35],\n",
       "       ...,\n",
       "       [1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.9616179e-25],\n",
       "       [1.0000000e+00, 1.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 1.0000000e+00, 3.5030451e-28, 2.0764496e-15]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequence_model(data):\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random_normal([data.shape[1], n_nodes_hl1])),\n",
    "                     'biases': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    output_layer = { 'weights': tf.Variable(tf.random_normal([n_nodes_hl1, n_sigmoid_output])),\n",
    "                    'biases': tf.Variable(tf.random_normal([n_sigmoid_output]))}\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    logits_output = tf.add(tf.matmul(l1, output_layer['weights']), output_layer['biases'])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
