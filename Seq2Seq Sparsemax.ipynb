{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "seed(42)\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "\n",
    "N_HIDDEN_HL1 = 10\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "seed(RANDOM_STATE)\n",
    "set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../data/imdb/imdb_original_preprocessed_ytest.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the vector by len = 80\n",
    "# k = 40\n",
    "\n",
    "word_list = []\n",
    "connotation = {}\n",
    "path = r'./imdb-unigrams.txt'\n",
    "\n",
    "with open(path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        word_list.append(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token_pattern = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(min_df = 5, token_pattern=token_pattern, lowercase=True, binary=True)\n",
    "X_train = cv.fit_transform(X_train_original)\n",
    "X_test = cv.transform(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure all the 'human-term' exists\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2)) \n",
    "\n",
    "words = intersection(cv.get_feature_names(), word_list)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_ht = CountVectorizer(token_pattern=token_pattern, vocabulary=word_list)\n",
    "X_train_ht = cv.fit_transform(X_train_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate log-ratio\n",
    "\n",
    "'''\n",
    "Count the negative and positive frequency\n",
    "'''\n",
    "def negative_positive_counts(X, y, word_index):\n",
    "    neg_count = np.sum(X[y==0, word_index])\n",
    "    pos_count = np.sum(X[y==1, word_index])    \n",
    "    return neg_count, pos_count\n",
    "\n",
    "'''\n",
    "Count the ratio : log(#pos/#neg)\n",
    "'''\n",
    "def log_ratio_positive_negative(X, y, word_index):\n",
    "    neg_count, pos_count = negative_positive_counts(X,y, word_index)\n",
    "    log_ratio = np.log(pos_count+1)-np.log(neg_count+1)\n",
    "    return log_ratio, neg_count, pos_count\n",
    "\n",
    "'''\n",
    "Sort top words w.r.t log ratio and write into file\n",
    "'''\n",
    "def sort_top_words_with_count(X, y, words,filename, top_k=10):\n",
    "    log_ratio = []\n",
    "    neg_count = []\n",
    "    pos_count = []\n",
    "    \n",
    "    for i in range(0,len(words)):\n",
    "        log_ratio_, neg_count_, pos_count_ = log_ratio_positive_negative(X, y, i)\n",
    "        log_ratio.append(log_ratio_)\n",
    "        neg_count.append(neg_count_)\n",
    "        pos_count.append(pos_count_)\n",
    "    \n",
    "    sorted_indices_descending_abs = np.argsort(np.absolute(log_ratio))[::-1]\n",
    "    \n",
    "    filename = filename + '.txt'\n",
    "    with open(filename, mode='w', encoding='utf8') as w:\n",
    "        for i in sorted_indices_descending_abs[: top_k]:\n",
    "#             print(\"%s\\t%0.2f\" %(words[i], weights[i]))\n",
    "#             n_p=negative_positive_counts(X, y, i)\n",
    "            w.write(\"%s\\t%0.2f\\t%d\\t%d\" %(str(words[i]), log_ratio[i], pos_count[i], neg_count[i]))\n",
    "            w.write('\\n')\n",
    "        w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_top_words_with_count(X_train_ht, y_train_original, word_list, 'human-terms-log-ratio', top_k=len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list(filename, split_delimiter):\n",
    "    vocabulary = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for l in f:\n",
    "            vocabulary.append(l.strip().split(split_delimiter))\n",
    "    return np.asarray(vocabulary)\n",
    "\n",
    "log_ratio_list = load_list('human-terms-log-ratio.txt', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2/10', '-3.87', '0', '47'],\n",
       "       ['annoying', '-3.15', '9', '233'],\n",
       "       ['badly', '-2.30', '0', '9'],\n",
       "       ['best', '2.27', '328', '33'],\n",
       "       ['funny', '1.99', '21', '2'],\n",
       "       ['solid', '1.95', '6', '0'],\n",
       "       ['waste', '1.95', '13', '1'],\n",
       "       ['fantastic', '1.79', '5', '0'],\n",
       "       ['awful', '-1.54', '2', '13'],\n",
       "       ['subtle', '1.39', '27', '6'],\n",
       "       ['8/10', '1.39', '7', '1'],\n",
       "       ['obnoxious', '-1.25', '1', '6'],\n",
       "       ['wasted', '1.25', '6', '1'],\n",
       "       ['worse', '1.25', '6', '1'],\n",
       "       ['1/10', '-1.15', '36', '116'],\n",
       "       ['insult', '-1.05', '20', '59'],\n",
       "       ['worst', '0.98', '7', '2'],\n",
       "       ['6/10', '0.92', '14', '5'],\n",
       "       ['7/10', '0.92', '4', '1'],\n",
       "       ['brilliant', '-0.92', '1', '4'],\n",
       "       ['forgettable', '-0.92', '1', '4'],\n",
       "       ['refreshing', '0.92', '4', '1'],\n",
       "       ['10/10', '0.92', '4', '1'],\n",
       "       ['disappointing', '0.92', '4', '1'],\n",
       "       ['unfortunately', '-0.92', '1', '4'],\n",
       "       ['5/10', '0.88', '11', '4'],\n",
       "       ['cheap', '-0.85', '14', '34'],\n",
       "       ['terrible', '-0.85', '2', '6'],\n",
       "       ['laughable', '0.83', '15', '6'],\n",
       "       ['predictable', '0.81', '8', '3'],\n",
       "       ['excellent', '0.79', '10', '4'],\n",
       "       ['amazing', '-0.73', '434', '899'],\n",
       "       ['9/10', '-0.69', '2', '5'],\n",
       "       ['mediocre', '0.69', '5', '2'],\n",
       "       ['boring', '-0.66', '14', '28'],\n",
       "       ['poor', '-0.64', '8', '16'],\n",
       "       ['avoid', '-0.63', '78', '148'],\n",
       "       ['noir', '0.62', '12', '6'],\n",
       "       ['gem', '0.54', '11', '6'],\n",
       "       ['perfectly', '-0.54', '6', '11'],\n",
       "       ['disappointed', '-0.51', '5', '9'],\n",
       "       ['enjoyable', '-0.51', '2', '4'],\n",
       "       ['stupid', '0.51', '4', '2'],\n",
       "       ['unfunny', '0.51', '4', '2'],\n",
       "       ['beautifully', '-0.51', '2', '4'],\n",
       "       ['surprisingly', '0.51', '4', '2'],\n",
       "       ['mst3k', '-0.51', '2', '4'],\n",
       "       ['dull', '-0.51', '2', '4'],\n",
       "       ['bad', '-0.47', '4', '7'],\n",
       "       ['fails', '0.44', '132', '85'],\n",
       "       ['lousy', '-0.43', '187', '287'],\n",
       "       ['wonderful', '0.41', '8', '5'],\n",
       "       ['great', '-0.41', '5', '8'],\n",
       "       ['4/10', '-0.34', '91', '128'],\n",
       "       ['pathetic', '0.34', '6', '4'],\n",
       "       ['dreadful', '-0.34', '4', '6'],\n",
       "       ['3/10', '-0.32', '34', '47'],\n",
       "       ['lacks', '0.30', '106', '78'],\n",
       "       ['sadly', '-0.29', '2', '3'],\n",
       "       ['redeeming', '0.29', '3', '2'],\n",
       "       ['poorly', '-0.29', '2', '3'],\n",
       "       ['fun', '-0.29', '2', '3'],\n",
       "       ['ridiculous', '-0.29', '2', '3'],\n",
       "       ['loved', '-0.25', '13', '17'],\n",
       "       ['recommended', '-0.22', '3', '4'],\n",
       "       ['disappointment', '0.22', '4', '3'],\n",
       "       ['beautiful', '-0.19', '601', '730'],\n",
       "       ['pointless', '0.17', '71', '60'],\n",
       "       ['bland', '-0.16', '181', '213'],\n",
       "       ['wonderfully', '-0.15', '5', '6'],\n",
       "       ['superb', '0.15', '35', '30'],\n",
       "       ['horrible', '-0.15', '106', '123'],\n",
       "       ['lame', '0.13', '7', '6'],\n",
       "       ['fascinating', '0.12', '8', '7'],\n",
       "       ['weak', '0.11', '9', '8'],\n",
       "       ['funniest', '0.04', '145', '139'],\n",
       "       ['mess', '0.03', '64', '62'],\n",
       "       ['perfect', '0.02', '82', '80'],\n",
       "       ['enjoyed', '0.00', '3', '3'],\n",
       "       ['favorite', '0.00', '3', '3'],\n",
       "       ['incredible', '0.00', '3', '3'],\n",
       "       ['rare', '0.00', '5', '5'],\n",
       "       ['tedious', '0.00', '3', '3']], dtype='<U14')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.expand_dims(X_train, axis=0)\n",
    "X_te = np.expand_dims(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = np.reshape(y_train_original, (len(y_train_original), 1))\n",
    "y_te = np.reshape(y_test_original, (len(y_test_original), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ht_tr = np.zeros([X_train.shape[0], X_train.shape[1]])\n",
    "y_ht_te = np.zeros([X_test.shape[0], X_train.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 26266)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ht_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe get intermediate layer of tanh h from Mitchell code.\n",
    "\n",
    "def load_unigrams(path, X, y):\n",
    "    word_list = []\n",
    "    connotation = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word_list.append(line.strip())\n",
    "            \n",
    "    for word in word_list:\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for i, doc in enumerate(X):\n",
    "            if word in doc.lower():\n",
    "                \n",
    "                if (y[i] == 1):\n",
    "                    pos_count += 1\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    \n",
    "        if pos_count > neg_count:\n",
    "            connotation[word] = 1\n",
    "        else:\n",
    "            connotation[word] = 0\n",
    "    \n",
    "    return word_list, connotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get function index\n",
    "\n",
    "vocabulary_ = cv.get_feature_names()\n",
    "vocab_index = {}\n",
    "\n",
    "for i,ht in enumerate(word_list):\n",
    "    for j, voc in enumerate(vocabulary_):\n",
    "        if voc == ht:\n",
    "            vocab_index[ht] = j\n",
    "            \n",
    "inv_voc = {v: k for k, v in vocab_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1/10': 11,\n",
       " '2/10': 185,\n",
       " '3/10': 225,\n",
       " '4/10': 250,\n",
       " '5/10': 268,\n",
       " '6/10': 285,\n",
       " '7/10': 302,\n",
       " '8/10': 320,\n",
       " '9/10': 338,\n",
       " '10/10': 18,\n",
       " 'amazing': 1065,\n",
       " 'annoying': 1262,\n",
       " 'avoid': 1858,\n",
       " 'awful': 1890,\n",
       " 'bad': 1976,\n",
       " 'badly': 1984,\n",
       " 'beautiful': 2276,\n",
       " 'beautifully': 2277,\n",
       " 'best': 2474,\n",
       " 'bland': 2656,\n",
       " 'boring': 2942,\n",
       " 'brilliant': 3096,\n",
       " 'cheap': 3956,\n",
       " 'disappointed': 6575,\n",
       " 'disappointing': 6576,\n",
       " 'disappointment': 6579,\n",
       " 'dreadful': 7107,\n",
       " 'dull': 7239,\n",
       " 'enjoyable': 7836,\n",
       " 'enjoyed': 7838,\n",
       " 'excellent': 8205,\n",
       " 'fails': 8481,\n",
       " 'fantastic': 8551,\n",
       " 'fascinating': 8589,\n",
       " 'favorite': 8638,\n",
       " 'forgettable': 9221,\n",
       " 'fun': 9496,\n",
       " 'funny': 9518,\n",
       " 'funniest': 9516,\n",
       " 'gem': 9709,\n",
       " 'great': 10202,\n",
       " 'horrible': 11280,\n",
       " 'incredible': 11863,\n",
       " 'insult': 12168,\n",
       " 'lacks': 13259,\n",
       " 'lame': 13292,\n",
       " 'laughable': 13399,\n",
       " 'lousy': 13996,\n",
       " 'loved': 14007,\n",
       " 'mediocre': 14778,\n",
       " 'mess': 14905,\n",
       " 'mst3k': 15498,\n",
       " 'noir': 16018,\n",
       " 'obnoxious': 16251,\n",
       " 'pathetic': 17057,\n",
       " 'perfect': 17246,\n",
       " 'perfectly': 17250,\n",
       " 'pointless': 17711,\n",
       " 'poor': 17787,\n",
       " 'poorly': 17790,\n",
       " 'predictable': 17980,\n",
       " 'rare': 18782,\n",
       " 'recommended': 18975,\n",
       " 'redeeming': 19018,\n",
       " 'refreshing': 19088,\n",
       " 'ridiculous': 19694,\n",
       " 'sadly': 20133,\n",
       " 'solid': 21684,\n",
       " 'stupid': 22551,\n",
       " 'subtle': 22626,\n",
       " 'superb': 22748,\n",
       " 'surprisingly': 22828,\n",
       " 'tedious': 23243,\n",
       " 'terrible': 23351,\n",
       " 'unfortunately': 24563,\n",
       " 'unfunny': 24569,\n",
       " 'waste': 25438,\n",
       " 'wasted': 25439,\n",
       " 'weak': 25494,\n",
       " 'wonderful': 25900,\n",
       " 'wonderfully': 25901,\n",
       " 'worse': 25957,\n",
       " 'worst': 25967}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_appearance(X_train, X_test, word_index, connotation):\n",
    "    y_train_agreement = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        doc_agreement = []\n",
    "        for w,j in word_index.items():\n",
    "            if X_train[i,j] == 1:\n",
    "                if connotation[w] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_train_agreement.append(doc_agreement)\n",
    "        \n",
    "    y_test_agreement = []\n",
    "    for i in range(X_test.shape[0]):\n",
    "        doc_agreement = []\n",
    "        for w,j in word_index.items():\n",
    "            if X_test[i,j] == 1:\n",
    "                if connotation[w] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_test_agreement.append(doc_agreement)\n",
    "        \n",
    "    return np.array(y_train_agreement), np.array(y_test_agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, connotation = load_unigrams('./imdb-unigrams.txt', X_train_original, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_agreement, y_test_agreement = generate_appearance(X_train, X_test, word_index=vocab_index, connotation=connotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ratio = {}\n",
    "for i in range(len(log_ratio_list)):\n",
    "    pos_ratio[log_ratio_list[i, 0]] = float(log_ratio_list[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ht_sum(y_agreement):\n",
    "    tr_ge2 = np.zeros(y_agreement.shape[0])\n",
    "\n",
    "    #X_reject_indices = np.squeeze(np.where(np.sum(np.absolute(y_agreement), axis=1)==0))\n",
    "    X_ge2_indices = np.squeeze(np.where(np.sum(np.absolute(y_agreement), axis=1)>1))\n",
    "    X_1_indices = np.squeeze(np.where(np.sum(np.absolute(y_agreement), axis=1)==1))\n",
    "\n",
    "    tr_ge2[X_ge2_indices] = 2\n",
    "    tr_ge2[X_1_indices] = 1\n",
    "    \n",
    "    return tr_ge2\n",
    "\n",
    "def where_sample_ht_index(y_agreement, pos_ratio, word_list):\n",
    "    \n",
    "    ht_ge_one = get_ht_sum(y_agreement)\n",
    "    \n",
    "    ht_sample_index = []\n",
    "    \n",
    "    for i,y in enumerate(ht_ge_one):\n",
    "        if y == 2:\n",
    "            indices = np.squeeze(np.where(y_agreement[i, :] != 0))\n",
    "\n",
    "            list_ = []\n",
    "            for j in indices:\n",
    "                try:\n",
    "                    list_.append(pos_ratio[word_list[j]])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "            list_ = np.asarray(np.absolute(list_))\n",
    "            ht_sample_index.append(indices[np.argmax(list_)])\n",
    "        elif y == 1:\n",
    "            ht_sample_index.append(np.where(y_agreement[i,:] != 0)[0][0])\n",
    "        else:\n",
    "            ht_sample_index.append(-1)\n",
    "            \n",
    "    return np.asarray(ht_sample_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence_sample(X_, y_agreement, pos_ratio, word_list, token_pattern=r\"(?u)\\b[\\w\\'/]+\\b\", input_seq_k=40, output_seq_t=2):\n",
    "    # get which human terms to be extracted\n",
    "    \n",
    "    ht_sample_index = where_sample_ht_index(y_agreement, \n",
    "                                            pos_ratio, \n",
    "                                            word_list)\n",
    "    X_sample = []\n",
    "    y_target = []\n",
    "\n",
    "    for idx, doc in enumerate(X_):\n",
    "        if ht_sample_index[idx] == -1:\n",
    "    #         X_sample.append('--NONE--')\n",
    "    #         y_target.append('--NONE--')\n",
    "    # discard doc with no human-terms\n",
    "            continue\n",
    "\n",
    "        join = '  '\n",
    "        target = '  ' \n",
    "        token = re.findall(token_pattern, doc)\n",
    "\n",
    "        for i,tok in enumerate(token):\n",
    "\n",
    "            if tok==word_list[ht_sample_index[idx]]:\n",
    "                # check if the length of document less than k\n",
    "                # then just use the whole document\n",
    "                if len(token) < input_seq_k and len(token)>(2*output_seq_t+1):\n",
    "                    join = ' '.join(token)\n",
    "                    target = ' '.join(token[i-output_seq_t:i+output_seq_t+1])\n",
    "                    break\n",
    "\n",
    "                # less than k, less than EOF\n",
    "                elif i < input_seq_k-1 and i<len(token)-1-input_seq_k:\n",
    "                    join = ' '.join(token[:i+input_seq_k+1])\n",
    "\n",
    "                    # define target\n",
    "                    if output_seq_t > i:\n",
    "                        target = ' '.join(token[:i+output_seq_t+1])\n",
    "                    else:\n",
    "                        target = ' '.join(token[i-output_seq_t:i+output_seq_t+1])\n",
    "\n",
    "                    break\n",
    "                # more than k, more than EOF\n",
    "                elif i>input_seq_k-1 and i>=len(token)-1-input_seq_k:\n",
    "                    join = ' '.join(token[i-input_seq_k:])\n",
    "\n",
    "                    #define target\n",
    "                    if output_seq_t >= len(token)-1-output_seq_t:\n",
    "                        target = ' '.join(token[i-output_seq_t:])\n",
    "\n",
    "                    else:\n",
    "                        target = ' '.join(token[i-output_seq_t:i+output_seq_t+1])\n",
    "\n",
    "                    break\n",
    "                else:\n",
    "                    join = ' '.join(token[i-input_seq_k:i+input_seq_k+1])\n",
    "                    target = ' '.join(token[i-output_seq_t:i+output_seq_t+1])\n",
    "                    break\n",
    "\n",
    "        X_sample.append(join)\n",
    "        y_target.append(target)\n",
    "    \n",
    "    return X_sample, y_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_sample, y_tr_target = generate_sequence_sample(X_train_original, y_train_agreement, pos_ratio, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_sample, y_te_target = generate_sequence_sample(X_test_original, y_test_agreement, pos_ratio, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tr_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'even kind of a happy ending of sort whee a step up from part 4 but not much of one again brian yuzna is involved and screaming mad george so some decent special effect but not enough to make this great a few leftover from part 4 are hanging around too like clint howard and neith hunter but that does not really make any difference anyway i now have seeing the whole series out of my system now if i could'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22701"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_te_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample generated\n",
    "#### Test on Seq2Seq architecture. \n",
    "\n",
    "Implement first in Keras <br>\n",
    "Preprocess the sequence using one-hot representation (omit the embedding for this stage) <br> <br>\n",
    "\n",
    "train, val, test : 25%, 25%, 50% <br> <br>\n",
    "\n",
    "<b>DO NOT MODIFY TEST SAMPLES</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# vectorize the data\n",
    "\n",
    "# https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/\n",
    "\n",
    "# count number of words with sets\n",
    "# or simply use the tokenizer update in Keras\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot\n",
    "\n",
    "train_sequence = []\n",
    "test_sequence = []\n",
    "for sample in X_tr_sample:\n",
    "    train_sequence.append(text_to_word_sequence(sample))\n",
    "for sample in X_te_sample:\n",
    "    test_sequence.append(text_to_word_sequence(sample))\n",
    "    \n",
    "train_target = []\n",
    "test_target = []\n",
    "\n",
    "for target in y_tr_target:\n",
    "    train_target.append(text_to_word_sequence(target))\n",
    "for target in y_te_target:\n",
    "    test_target.append(text_to_word_sequence(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22701"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequence)\n",
    "len(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So let preprocess it with one hot which provided by Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequence[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS = 10000\n",
    "t = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "\n",
    "t.fit_on_texts(X_tr_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_sequence = t.texts_to_sequences(X_tr_sample)\n",
    "X_te_sequence = t.texts_to_sequences(X_te_sample)\n",
    "\n",
    "y_tr_sequence = t.texts_to_sequences(y_tr_target)\n",
    "y_te_sequence = t.texts_to_sequences(y_te_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can't use one-hot.\n",
    "# Use embedding instead\n",
    "len(train_sequence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tensor\n",
    "MAX_ENCODER_SEQUENCE = len(train_sequence[0])\n",
    "MAX_DECODER_SEQUENCE = len(train_target[0])\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = (1./3)\n",
    "\n",
    "X_tensor_data = np.zeros((len(X_tr_sample), MAX_ENCODER_SEQUENCE, EMBEDDING_DIM), dtype='int32')\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(X_tr_sample), MAX_ENCODER_SEQUENCE, EMBEDDING_DIM))\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(y_tr_target), MAX_DECODER_SEQUENCE, EMBEDDING_DIM))\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(y_tr_target), MAX_DECODER_SEQUENCE, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "## Initiate Embedding layer\n",
    "\n",
    "import os\n",
    "from keras.layers import Embedding\n",
    "\n",
    "GLOVE_DIR = \"../data/glove.6B\"\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), 'rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE = 80\n",
    "\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                               EMBEDDING_DIM,\n",
    "                               weights=[embedding_matrix],\n",
    "                               input_length=MAX_SEQUENCE,\n",
    "                               trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "long_input_sequence = Input(shape=(MAX_SEQUENCE,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(long_input_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(long_input_sequence, embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 80)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 80, 100)           4240700   \n",
      "=================================================================\n",
      "Total params: 4,240,700\n",
      "Trainable params: 4,240,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Encoder and decoder first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
