{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4844281504857694146\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 121241600\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 3301149952505954236\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = open_pickle('../../data/imdb_sequence/3000_one_hot/X_tr_seq_set.pkl')\n",
    "y = open_pickle('../../data/imdb_sequence/3000_one_hot/y_tr_seq_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'padded'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22752, 80)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['padded'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X['padded'])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'padded'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer\n",
    "MAX_NUM_WORDS = 1000\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 100\n",
    "NUM_ENCODER_TOKENS = np.max(X['padded']+1)\n",
    "NUM_DECODER_TOKENS = np.max(X['padded']+1)\n",
    "max_encoder_seq_length = X['padded'].shape[1]\n",
    "max_decoder_seq_length = X['padded'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (X['padded'].shape[0], max_encoder_seq_length, NUM_ENCODER_TOKENS),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (y['padded'].shape[0], max_decoder_seq_length, NUM_DECODER_TOKENS),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (y['padded'].shape[0], max_decoder_seq_length, NUM_DECODER_TOKENS),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22752, 80, 3000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate input/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(X['padded'], y['padded'])):\n",
    "    for t, word in enumerate(input_text):\n",
    "        encoder_input_data[i, t, word] = 1.\n",
    "        \n",
    "    for t, word in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, word] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, word] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "\n",
    "encoder_inputs = Input(shape=(None, NUM_ENCODER_TOKENS))\n",
    "encoder = LSTM(LATENT_DIM, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, NUM_DECODER_TOKENS))\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(NUM_DECODER_TOKENS, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "22752/22752 [==============================] - 124s 5ms/step - loss: 0.1041\n",
      "Epoch 2/500\n",
      "22752/22752 [==============================] - 124s 5ms/step - loss: 0.1035\n",
      "Epoch 3/500\n",
      "22752/22752 [==============================] - 125s 5ms/step - loss: 0.1031\n",
      "Epoch 4/500\n",
      "22752/22752 [==============================] - 125s 6ms/step - loss: 0.1025\n",
      "Epoch 5/500\n",
      "22752/22752 [==============================] - 125s 5ms/step - loss: 0.1023\n",
      "Epoch 6/500\n",
      "22752/22752 [==============================] - 125s 5ms/step - loss: 0.1017\n",
      "Epoch 7/500\n",
      "22752/22752 [==============================] - 125s 5ms/step - loss: 0.1014\n",
      "Epoch 8/500\n",
      "22752/22752 [==============================] - 125s 5ms/step - loss: 0.1012\n",
      "Epoch 9/500\n",
      "22752/22752 [==============================] - 125s 6ms/step - loss: 0.1008\n",
      "Epoch 10/500\n",
      "22752/22752 [==============================] - 123s 5ms/step - loss: 0.1004\n",
      "Epoch 11/500\n",
      "22752/22752 [==============================] - 125s 5ms/step - loss: 0.1005\n",
      "Epoch 12/500\n",
      "22752/22752 [==============================] - 124s 5ms/step - loss: 0.1001\n",
      "Epoch 13/500\n",
      "22752/22752 [==============================] - 124s 5ms/step - loss: 0.0998\n",
      "Epoch 14/500\n",
      "22752/22752 [==============================] - 125s 5ms/step - loss: 0.0998\n",
      "Epoch 15/500\n",
      "22752/22752 [==============================] - 122s 5ms/step - loss: 0.0995\n",
      "Epoch 16/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0991\n",
      "Epoch 17/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0991\n",
      "Epoch 18/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0990\n",
      "Epoch 19/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0987\n",
      "Epoch 20/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0987\n",
      "Epoch 21/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0986\n",
      "Epoch 22/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0985\n",
      "Epoch 23/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0985\n",
      "Epoch 24/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0984\n",
      "Epoch 25/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0983\n",
      "Epoch 26/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0986\n",
      "Epoch 27/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0981\n",
      "Epoch 28/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0992\n",
      "Epoch 29/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0985\n",
      "Epoch 30/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0981\n",
      "Epoch 31/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0977\n",
      "Epoch 32/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0982\n",
      "Epoch 33/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0977\n",
      "Epoch 34/500\n",
      "22752/22752 [==============================] - 127s 6ms/step - loss: 0.0972\n",
      "Epoch 35/500\n",
      "22752/22752 [==============================] - 126s 6ms/step - loss: 0.0973\n",
      "Epoch 36/500\n",
      "12544/22752 [===============>..............] - ETA: 56s - loss: 0.0971"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-830df437169d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anneke/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "\n",
    "# model.save('3000_one_hot_s2s.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling models\n",
    "\n",
    "# https://nlp.stanford.edu/~johnhew/public/14-seq2seq.pdf\n",
    "# https://medium.com/machine-learning-bites/deeplearning-series-sequence-to-sequence-architectures-4c4ca89e5654\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                     [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict((i,word) for word,i in word_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_list(X):\n",
    "    y_candidate = []\n",
    "    \n",
    "    for i in range(X.shape[0]-1-5):\n",
    "        y_candidate.append(X[i:i+5])\n",
    "    \n",
    "    return np.asarray(y_candidate)\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "\n",
    "def target_index(doc_idx, candidate_seq, y):\n",
    "    for i,j in enumerate(candidate_seq):\n",
    "        if len(intersection(j, y)) == len(y):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "# doc num, doc index argmax\n",
    "\n",
    "def to_sequence(int_sequence):\n",
    "    decoded = ''\n",
    "    for i in int_sequence:\n",
    "        if i == 0:\n",
    "            word = ' '\n",
    "        else:\n",
    "            word = reverse_word_index[i]\n",
    "        decoded += word + ' '\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with candidate\n",
    "\n",
    "def decode_sequence_target(candidate_states_value, candidate_target_seq):\n",
    "#     candidate_states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    from_candidate_target_seq = np.zeros((1,1, NUM_DECODER_TOKENS))\n",
    "    candidate_token_index = candidate_target_seq[0,0]\n",
    "    from_candidate_target_seq[0,0,candidate_token_index] = 1.\n",
    "    candidate_joint_log_prob = 0\n",
    "    \n",
    "    for i in range(1,5):\n",
    "        from_candidate_output_tokens, h_true, c_true = decoder_model.predict([from_candidate_target_seq] + candidate_states_value)\n",
    "    \n",
    "        candidate_target_prob = from_candidate_output_tokens[0,-1, candidate_target_seq[0,i]]\n",
    "        candidate_token_index = candidate_target_seq[0,i]\n",
    "        candidate_joint_log_prob += np.log(candidate_target_prob)\n",
    "        \n",
    "        # get the t+1 input\n",
    "        from_candidate_target_seq = np.zeros((1,1,NUM_DECODER_TOKENS))\n",
    "        from_candidate_target_seq[0,0,candidate_token_index] = 1.\n",
    "        \n",
    "        candidate_states_value = [h_true, c_true]\n",
    "\n",
    "    return candidate_joint_log_prob, candidate_target_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "file = open(\"candidate_jll_3000_one_hot_all.csv\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anneke/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 100...\n",
      "Processing document 200...\n",
      "Processing document 300...\n",
      "Processing document 400...\n",
      "Processing document 500...\n",
      "Processing document 600...\n",
      "Processing document 700...\n",
      "Processing document 800...\n",
      "Processing document 900...\n",
      "Processing document 1000...\n",
      "Processing document 1100...\n",
      "Processing document 1200...\n",
      "Processing document 1300...\n",
      "Processing document 1400...\n",
      "Processing document 1500...\n",
      "Processing document 1600...\n",
      "Processing document 1700...\n",
      "Processing document 1800...\n",
      "Processing document 1900...\n",
      "Processing document 2000...\n",
      "Processing document 2100...\n",
      "Processing document 2200...\n",
      "Processing document 2300...\n",
      "Processing document 2400...\n",
      "Processing document 2500...\n",
      "Processing document 2600...\n",
      "Processing document 2700...\n",
      "Processing document 2800...\n",
      "Processing document 2900...\n",
      "Processing document 3000...\n",
      "Processing document 3100...\n",
      "Processing document 3200...\n",
      "Processing document 3300...\n",
      "Processing document 3400...\n",
      "Processing document 3500...\n",
      "Processing document 3600...\n",
      "Processing document 3700...\n",
      "Processing document 3800...\n",
      "Processing document 3900...\n",
      "Processing document 4000...\n",
      "Processing document 4100...\n",
      "Processing document 4200...\n",
      "Processing document 4300...\n",
      "Processing document 4400...\n",
      "Processing document 4500...\n",
      "Processing document 4600...\n",
      "Processing document 4700...\n",
      "Processing document 4800...\n",
      "Processing document 4900...\n",
      "Processing document 5000...\n",
      "Processing document 5100...\n",
      "Processing document 5200...\n",
      "Processing document 5300...\n",
      "Processing document 5400...\n",
      "Processing document 5500...\n",
      "Processing document 5600...\n",
      "Processing document 5700...\n",
      "Processing document 5800...\n",
      "Processing document 5900...\n",
      "Processing document 6000...\n",
      "Processing document 6100...\n",
      "Processing document 6200...\n",
      "Processing document 6300...\n",
      "Processing document 6400...\n",
      "Processing document 6500...\n",
      "Processing document 6600...\n",
      "Processing document 6700...\n",
      "Processing document 6800...\n",
      "Processing document 6900...\n",
      "Processing document 7000...\n",
      "Processing document 7100...\n",
      "Processing document 7200...\n",
      "Processing document 7300...\n",
      "Processing document 7400...\n",
      "Processing document 7500...\n",
      "Processing document 7600...\n",
      "Processing document 7700...\n",
      "Processing document 7800...\n",
      "Processing document 7900...\n",
      "Processing document 8000...\n",
      "Processing document 8100...\n",
      "Processing document 8200...\n",
      "Processing document 8300...\n",
      "Processing document 8400...\n",
      "Processing document 8500...\n",
      "Processing document 8600...\n",
      "Processing document 8700...\n",
      "Processing document 8800...\n",
      "Processing document 8900...\n",
      "Processing document 9000...\n",
      "Processing document 9100...\n",
      "Processing document 9200...\n",
      "Processing document 9300...\n",
      "Processing document 9400...\n",
      "Processing document 9500...\n",
      "Processing document 9600...\n",
      "Processing document 9700...\n",
      "Processing document 9800...\n",
      "Processing document 9900...\n",
      "Processing document 10000...\n",
      "Processing document 10100...\n",
      "Processing document 10200...\n",
      "Processing document 10300...\n",
      "Processing document 10400...\n",
      "Processing document 10500...\n",
      "Processing document 10600...\n",
      "Processing document 10700...\n",
      "Processing document 10800...\n",
      "Processing document 10900...\n",
      "Processing document 11000...\n",
      "Processing document 11100...\n",
      "Processing document 11200...\n",
      "Processing document 11300...\n",
      "Processing document 11400...\n",
      "Processing document 11500...\n",
      "Processing document 11600...\n",
      "Processing document 11700...\n",
      "Processing document 11800...\n",
      "Processing document 11900...\n",
      "Processing document 12000...\n",
      "Processing document 12100...\n",
      "Processing document 12200...\n",
      "Processing document 12300...\n",
      "Processing document 12400...\n",
      "Processing document 12500...\n",
      "Processing document 12600...\n",
      "Processing document 12700...\n",
      "Processing document 12800...\n",
      "Processing document 12900...\n",
      "Processing document 13000...\n",
      "Processing document 13100...\n",
      "Processing document 13200...\n",
      "Processing document 13300...\n",
      "Processing document 13400...\n",
      "Processing document 13500...\n",
      "Processing document 13600...\n",
      "Processing document 13700...\n",
      "Processing document 13800...\n",
      "Processing document 13900...\n",
      "Processing document 14000...\n",
      "Processing document 14100...\n",
      "Processing document 14200...\n",
      "Processing document 14300...\n",
      "Processing document 14400...\n",
      "Processing document 14500...\n",
      "Processing document 14600...\n",
      "Processing document 14700...\n",
      "Processing document 14800...\n",
      "Processing document 14900...\n",
      "Processing document 15000...\n",
      "Processing document 15100...\n",
      "Processing document 15200...\n",
      "Processing document 15300...\n",
      "Processing document 15400...\n",
      "Processing document 15500...\n",
      "Processing document 15600...\n",
      "Processing document 15700...\n",
      "Processing document 15800...\n",
      "Processing document 15900...\n",
      "Processing document 16000...\n",
      "Processing document 16100...\n",
      "Processing document 16200...\n",
      "Processing document 16300...\n",
      "Processing document 16400...\n",
      "Processing document 16500...\n",
      "Processing document 16600...\n",
      "Processing document 16700...\n",
      "Processing document 16800...\n",
      "Processing document 16900...\n",
      "Processing document 17000...\n",
      "Processing document 17100...\n",
      "Processing document 17200...\n",
      "Processing document 17300...\n",
      "Processing document 17400...\n",
      "Processing document 17500...\n",
      "Processing document 17600...\n",
      "Processing document 17700...\n",
      "Processing document 17800...\n",
      "Processing document 17900...\n",
      "Processing document 18000...\n",
      "Processing document 18100...\n",
      "Processing document 18200...\n",
      "Processing document 18300...\n",
      "Processing document 18400...\n",
      "Processing document 18500...\n",
      "Processing document 18600...\n",
      "Processing document 18700...\n",
      "Processing document 18800...\n",
      "Processing document 18900...\n",
      "Processing document 19000...\n",
      "Processing document 19100...\n",
      "Processing document 19200...\n",
      "Processing document 19300...\n",
      "Processing document 19400...\n",
      "Processing document 19500...\n",
      "Processing document 19600...\n",
      "Processing document 19700...\n",
      "Processing document 19800...\n",
      "Processing document 19900...\n",
      "Processing document 20000...\n",
      "Processing document 20100...\n",
      "Processing document 20200...\n",
      "Processing document 20300...\n",
      "Processing document 20400...\n",
      "Processing document 20500...\n",
      "Processing document 20600...\n",
      "Processing document 20700...\n",
      "Processing document 20800...\n",
      "Processing document 20900...\n",
      "Processing document 21000...\n",
      "Processing document 21100...\n",
      "Processing document 21200...\n",
      "Processing document 21300...\n",
      "Processing document 21400...\n",
      "Processing document 21500...\n",
      "Processing document 21600...\n",
      "Processing document 21700...\n",
      "Processing document 21800...\n",
      "Processing document 21900...\n",
      "Processing document 22000...\n",
      "Processing document 22100...\n",
      "Processing document 22200...\n",
      "Processing document 22300...\n",
      "Processing document 22400...\n",
      "Processing document 22500...\n",
      "Processing document 22600...\n",
      "Processing document 22700...\n"
     ]
    }
   ],
   "source": [
    "for doc in X['padded']:\n",
    "    y_candidate = generate_candidate_list(doc)\n",
    "    \n",
    "    candidate_jll_per_doc = []\n",
    "    input_seq = encoder_input_data[i:i+1]\n",
    "    \n",
    "    true_target_index = target_index(i, y_candidate, y['padded'][i])\n",
    "    \n",
    "    # Encode\n",
    "    candidate_states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    for j in range(y_candidate.shape[0]):\n",
    "        candidate_seq = y_candidate[j:j+1]\n",
    "        candidate_jll_slide, candidate_last_prob = decode_sequence_target(candidate_states_value, candidate_seq)\n",
    "        candidate_jll_per_doc.append(candidate_jll_slide)\n",
    "\n",
    "    candidate_jll_per_doc = np.asarray(candidate_jll_per_doc)\n",
    "    max_jll_index = np.argmax(candidate_jll_per_doc)\n",
    "    true_target_jll = np.around(candidate_jll_per_doc[true_target_index],5)\n",
    "    max_candidate_jll = np.around(candidate_jll_per_doc[max_jll_index],5)\n",
    "    \n",
    "    \n",
    "    file.write('%d\\t%d\\t%s\\t%d\\t%s\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%d\\t%.5f\\t%.5f\\n' %(i, true_target_index, y['text'][i],\n",
    "                                                            max_jll_index, to_sequence(y_candidate[max_jll_index]),\n",
    "                                                            -(true_target_index-max_jll_index),\n",
    "                                                            true_target_jll, max_candidate_jll,\n",
    "                                                            np.absolute(true_target_jll-max_candidate_jll),\n",
    "                                                            len(intersection(y['padded'][i], y_candidate[max_jll_index])),\n",
    "                                                            np.exp(true_target_jll/4), np.exp(max_candidate_jll/4)))\n",
    "    \n",
    "#     print('%d\\t%d\\t%s\\t%d\\t%s\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%d\\n' %(i, true_target_index, y['text'][i],\n",
    "#                                                             max_jll_index, to_sequence(y_candidate[max_jll_index]),\n",
    "#                                                             -(true_target_index-max_jll_index),\n",
    "#                                                             true_target_jll, max_candidate_jll,\n",
    "#                                                             np.absolute(true_target_jll-max_candidate_jll),\n",
    "#                                                             len(intersection(y['padded'][i], y_candidate[max_jll_index]))))\n",
    "    if i % 100 == 0:\n",
    "        print('Processing document %d...' %(i))\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start test data preprocessing\n",
    "\n",
    "X_te = open_pickle('../../data/imdb_sequence/3000_one_hot/X_te_seq_set.pkl')\n",
    "y_te = open_pickle('../../data/imdb_sequence/3000_one_hot/y_te_seq_set.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te['padded'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoder_input_data = np.zeros(\n",
    "    (X['padded'].shape[0], max_encoder_seq_length+5, NUM_ENCODER_TOKENS),\n",
    "    dtype='float32')\n",
    "test_decoder_input_data = np.zeros(\n",
    "    (y['padded'].shape[0], max_decoder_seq_length+5, NUM_DECODER_TOKENS),\n",
    "    dtype='float32')\n",
    "test_decoder_target_data = np.zeros(\n",
    "    (y['padded'].shape[0], max_decoder_seq_length+5, NUM_DECODER_TOKENS),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(X_te['padded'], y_te['padded'])):\n",
    "    for t, word in enumerate(input_text):\n",
    "        test_encoder_input_data[i, t, word] = 1.\n",
    "        \n",
    "    for t, word in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        test_decoder_input_data[i, t, word] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            test_decoder_target_data[i, t - 1, word] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "file = open(\"test_candidate_jll_3000_one_hot_all.csv\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anneke/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 100...\n",
      "Processing document 200...\n",
      "Processing document 300...\n",
      "Processing document 400...\n",
      "Processing document 500...\n",
      "Processing document 600...\n",
      "Processing document 700...\n",
      "Processing document 800...\n",
      "Processing document 900...\n",
      "Processing document 1000...\n",
      "Processing document 1100...\n",
      "Processing document 1200...\n",
      "Processing document 1300...\n",
      "Processing document 1400...\n",
      "Processing document 1500...\n",
      "Processing document 1600...\n",
      "Processing document 1700...\n",
      "Processing document 1800...\n",
      "Processing document 1900...\n",
      "Processing document 2000...\n",
      "Processing document 2100...\n",
      "Processing document 2200...\n",
      "Processing document 2300...\n",
      "Processing document 2400...\n",
      "Processing document 2500...\n",
      "Processing document 2600...\n",
      "Processing document 2700...\n",
      "Processing document 2800...\n",
      "Processing document 2900...\n",
      "Processing document 3000...\n",
      "Processing document 3100...\n",
      "Processing document 3200...\n",
      "Processing document 3300...\n",
      "Processing document 3400...\n",
      "Processing document 3500...\n",
      "Processing document 3600...\n",
      "Processing document 3700...\n",
      "Processing document 3800...\n",
      "Processing document 3900...\n",
      "Processing document 4000...\n",
      "Processing document 4100...\n",
      "Processing document 4200...\n",
      "Processing document 4300...\n",
      "Processing document 4400...\n",
      "Processing document 4500...\n",
      "Processing document 4600...\n",
      "Processing document 4700...\n",
      "Processing document 4800...\n",
      "Processing document 4900...\n",
      "Processing document 5000...\n",
      "Processing document 5100...\n",
      "Processing document 5200...\n",
      "Processing document 5300...\n",
      "Processing document 5400...\n",
      "Processing document 5500...\n",
      "Processing document 5600...\n",
      "Processing document 5700...\n",
      "Processing document 5800...\n",
      "Processing document 5900...\n",
      "Processing document 6000...\n",
      "Processing document 6100...\n",
      "Processing document 6200...\n",
      "Processing document 6300...\n",
      "Processing document 6400...\n",
      "Processing document 6500...\n",
      "Processing document 6600...\n",
      "Processing document 6700...\n",
      "Processing document 6800...\n",
      "Processing document 6900...\n",
      "Processing document 7000...\n",
      "Processing document 7100...\n",
      "Processing document 7200...\n",
      "Processing document 7300...\n",
      "Processing document 7400...\n",
      "Processing document 7500...\n",
      "Processing document 7600...\n",
      "Processing document 7700...\n",
      "Processing document 7800...\n",
      "Processing document 7900...\n",
      "Processing document 8000...\n",
      "Processing document 8100...\n",
      "Processing document 8200...\n",
      "Processing document 8300...\n",
      "Processing document 8400...\n",
      "Processing document 8500...\n",
      "Processing document 8600...\n",
      "Processing document 8700...\n",
      "Processing document 8800...\n",
      "Processing document 8900...\n",
      "Processing document 9000...\n",
      "Processing document 9100...\n",
      "Processing document 9200...\n",
      "Processing document 9300...\n",
      "Processing document 9400...\n",
      "Processing document 9500...\n",
      "Processing document 9600...\n",
      "Processing document 9700...\n",
      "Processing document 9800...\n",
      "Processing document 9900...\n",
      "Processing document 10000...\n",
      "Processing document 10100...\n",
      "Processing document 10200...\n",
      "Processing document 10300...\n",
      "Processing document 10400...\n",
      "Processing document 10500...\n",
      "Processing document 10600...\n",
      "Processing document 10700...\n",
      "Processing document 10800...\n",
      "Processing document 10900...\n",
      "Processing document 11000...\n",
      "Processing document 11100...\n",
      "Processing document 11200...\n",
      "Processing document 11300...\n",
      "Processing document 11400...\n",
      "Processing document 11500...\n",
      "Processing document 11600...\n",
      "Processing document 11700...\n",
      "Processing document 11800...\n",
      "Processing document 11900...\n",
      "Processing document 12000...\n",
      "Processing document 12100...\n",
      "Processing document 12200...\n",
      "Processing document 12300...\n",
      "Processing document 12400...\n",
      "Processing document 12500...\n",
      "Processing document 12600...\n",
      "Processing document 12700...\n",
      "Processing document 12800...\n",
      "Processing document 12900...\n",
      "Processing document 13000...\n",
      "Processing document 13100...\n",
      "Processing document 13200...\n",
      "Processing document 13300...\n",
      "Processing document 13400...\n",
      "Processing document 13500...\n",
      "Processing document 13600...\n",
      "Processing document 13700...\n",
      "Processing document 13800...\n",
      "Processing document 13900...\n",
      "Processing document 14000...\n",
      "Processing document 14100...\n",
      "Processing document 14200...\n",
      "Processing document 14300...\n",
      "Processing document 14400...\n",
      "Processing document 14500...\n",
      "Processing document 14600...\n",
      "Processing document 14700...\n",
      "Processing document 14800...\n",
      "Processing document 14900...\n",
      "Processing document 15000...\n",
      "Processing document 15100...\n",
      "Processing document 15200...\n",
      "Processing document 15300...\n",
      "Processing document 15400...\n",
      "Processing document 15500...\n",
      "Processing document 15600...\n",
      "Processing document 15700...\n",
      "Processing document 15800...\n",
      "Processing document 15900...\n",
      "Processing document 16000...\n",
      "Processing document 16100...\n",
      "Processing document 16200...\n",
      "Processing document 16300...\n",
      "Processing document 16400...\n",
      "Processing document 16500...\n",
      "Processing document 16600...\n",
      "Processing document 16700...\n",
      "Processing document 16800...\n",
      "Processing document 16900...\n",
      "Processing document 17000...\n",
      "Processing document 17100...\n",
      "Processing document 17200...\n",
      "Processing document 17300...\n",
      "Processing document 17400...\n",
      "Processing document 17500...\n",
      "Processing document 17600...\n",
      "Processing document 17700...\n",
      "Processing document 17800...\n",
      "Processing document 17900...\n",
      "Processing document 18000...\n",
      "Processing document 18100...\n",
      "Processing document 18200...\n",
      "Processing document 18300...\n",
      "Processing document 18400...\n",
      "Processing document 18500...\n",
      "Processing document 18600...\n",
      "Processing document 18700...\n",
      "Processing document 18800...\n",
      "Processing document 18900...\n",
      "Processing document 19000...\n",
      "Processing document 19100...\n",
      "Processing document 19200...\n",
      "Processing document 19300...\n",
      "Processing document 19400...\n",
      "Processing document 19500...\n",
      "Processing document 19600...\n",
      "Processing document 19700...\n",
      "Processing document 19800...\n",
      "Processing document 19900...\n",
      "Processing document 20000...\n",
      "Processing document 20100...\n",
      "Processing document 20200...\n",
      "Processing document 20300...\n",
      "Processing document 20400...\n",
      "Processing document 20500...\n",
      "Processing document 20600...\n",
      "Processing document 20700...\n",
      "Processing document 20800...\n",
      "Processing document 20900...\n",
      "Processing document 21000...\n",
      "Processing document 21100...\n",
      "Processing document 21200...\n",
      "Processing document 21300...\n",
      "Processing document 21400...\n",
      "Processing document 21500...\n",
      "Processing document 21600...\n",
      "Processing document 21700...\n",
      "Processing document 21800...\n",
      "Processing document 21900...\n",
      "Processing document 22000...\n",
      "Processing document 22100...\n",
      "Processing document 22200...\n",
      "Processing document 22300...\n",
      "Processing document 22400...\n",
      "Processing document 22500...\n",
      "Processing document 22600...\n",
      "Processing document 22700...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 22701 is out of bounds for axis 0 with size 22701",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-a1c93c7f49a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_input_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrue_target_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_candidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_te\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'padded'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 22701 is out of bounds for axis 0 with size 22701"
     ]
    }
   ],
   "source": [
    "for doc in X_te['padded']:\n",
    "    y_candidate = generate_candidate_list(doc)\n",
    "    \n",
    "    candidate_jll_per_doc = []\n",
    "    input_seq = encoder_input_data[i:i+1]\n",
    "    \n",
    "    true_target_index = target_index(i, y_candidate, y_te['padded'][i])\n",
    "    \n",
    "    # Encode\n",
    "    candidate_states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    for j in range(y_candidate.shape[0]):\n",
    "        candidate_seq = y_candidate[j:j+1]\n",
    "        candidate_jll_slide, candidate_last_prob = decode_sequence_target(candidate_states_value, candidate_seq)\n",
    "        candidate_jll_per_doc.append(candidate_jll_slide)\n",
    "\n",
    "    candidate_jll_per_doc = np.asarray(candidate_jll_per_doc)\n",
    "    max_jll_index = np.argmax(candidate_jll_per_doc)\n",
    "    true_target_jll = np.around(candidate_jll_per_doc[true_target_index],5)\n",
    "    max_candidate_jll = np.around(candidate_jll_per_doc[max_jll_index],5)\n",
    "    \n",
    "    \n",
    "    file.write('%d\\t%d\\t%s\\t%d\\t%s\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%d\\t%.5f\\t%.5f\\n' %(i, true_target_index, y_te['text'][i],\n",
    "                                                            max_jll_index, to_sequence(y_candidate[max_jll_index]),\n",
    "                                                            -(true_target_index-max_jll_index),\n",
    "                                                            true_target_jll, max_candidate_jll,\n",
    "                                                            np.absolute(true_target_jll-max_candidate_jll),\n",
    "                                                            len(intersection(y_te['padded'][i], y_candidate[max_jll_index])),\n",
    "                                                            np.exp(true_target_jll/4), np.exp(max_candidate_jll/4)))\n",
    "    \n",
    "#     print('%d\\t%d\\t%s\\t%d\\t%s\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%d\\n' %(i, true_target_index, y['text'][i],\n",
    "#                                                             max_jll_index, to_sequence(y_candidate[max_jll_index]),\n",
    "#                                                             -(true_target_index-max_jll_index),\n",
    "#                                                             true_target_jll, max_candidate_jll,\n",
    "#                                                             np.absolute(true_target_jll-max_candidate_jll),\n",
    "#                                                             len(intersection(y['padded'][i], y_candidate[max_jll_index]))))\n",
    "    if i % 100 == 0:\n",
    "        print('Processing document %d...' %(i))\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
