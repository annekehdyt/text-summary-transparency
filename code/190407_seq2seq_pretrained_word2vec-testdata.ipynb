{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import gensim\n",
    "import keras as k\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "\n",
    "import util as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = tf.ConfigProto(device_count={\"CPU\": 8})\n",
    "# k.backend.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Slacker Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to anneke@iitml.\n"
     ]
    }
   ],
   "source": [
    "slack = u.initiate_slacker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Google's pretrained word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../../data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sequence = u.open_pickle('../../data/imdb/X_tr_sample_original.pkl')\n",
    "X_test_sequence = u.open_pickle('../../data/imdb/X_te_sample_original.pkl')\n",
    "y_train_target = u.open_pickle('../../data/imdb/y_tr_target_original.pkl')\n",
    "y_test_target = u.open_pickle('../../data/imdb/y_te_target_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_target = [' '.join(['<UNK>', y]) for y in y_train_target]\n",
    "y_test_target = [' '.join(['<UNK>', y]) for y in y_test_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize constant here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ENCODER_SEQ_LEN = 81\n",
    "MAX_DECODER_SEQ_LEN = 6 #include <UNK>\n",
    "EMBEDDING_DIM = 300\n",
    "LATENT_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the train sequence data\n",
    "tokenizer = k.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train_sequence)\n",
    "\n",
    "# Generate text to integer sequence with post padding\n",
    "# X_tr_padded = pad_sequences(tokenizer.texts_to_sequences(X_train_sequence), maxlen=81, padding='post', truncating='post')\n",
    "# y_tr_padded = pad_sequences(tokenizer.texts_to_sequences(y_train_target), maxlen=5, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tr_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DECODER_TOKENS = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {v: k for k, v in tokenizer.word_index.items()}\n",
    "index_to_word[0] = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42406"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_DECODER_TOKENS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the container for input sequence decoder and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(X_train_sequence), MAX_ENCODER_SEQ_LEN, EMBEDDING_DIM),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(y_train_target), MAX_DECODER_SEQ_LEN, EMBEDDING_DIM),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(y_train_target), MAX_DECODER_SEQ_LEN, len(tokenizer.word_index)),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_sequence, target_sequence) in enumerate(zip(X_tr_padded, y_tr_padded)):\n",
    "    # embed the input sequence\n",
    "    for t, index in enumerate(input_sequence):\n",
    "        try:\n",
    "            encoder_input_data[i, t, :] = w2v_model[index_to_word[index]]\n",
    "        except KeyError as error:\n",
    "            pass\n",
    "    \n",
    "    # embed the input decoder\n",
    "    for t, index in enumerate(target_sequence):\n",
    "        try:\n",
    "            decoder_input_data[i, t, :] = w2v_model[index_to_word[index]]\n",
    "        except KeyError as error:\n",
    "            pass\n",
    "        \n",
    "    for t, index in enumerate(target_sequence):\n",
    "        # not include the first <UNK>\n",
    "        if t>0:\n",
    "            decoder_target_data[i, t - 1, index] = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = k.callbacks.ModelCheckpoint(save_best_only=True, monitor='val_loss', filepath='./300_word2vec_best_model/weights.{epoch:04d}-{val_loss:.3f}.h5')\n",
    "# csvlogger = k.callbacks.CSVLogger(filename='word2vec_300_history.log', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = k.layers.Input(shape=(None, EMBEDDING_DIM))\n",
    "# encoder = LSTM(LATENT_DIM, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder_inputs = k.layers.Input(shape=(None, EMBEDDING_DIM))\n",
    "# decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "# decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "#                                     initial_state=encoder_states)\n",
    "\n",
    "# decoder_dense = k.layers.Dense(NUM_DECODER_TOKENS, activation='softmax')\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model = k.models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# train_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs=500\n",
    "# batch_size=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15168 samples, validate on 7584 samples\n",
      "Epoch 1/500\n",
      "15168/15168 [==============================] - 402s 27ms/step - loss: 5.3539 - val_loss: 3.9570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anneke/.local/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.8193 - val_loss: 3.8857\n",
      "Epoch 3/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.7246 - val_loss: 3.8293\n",
      "Epoch 4/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.6572 - val_loss: 3.7967\n",
      "Epoch 5/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.6118 - val_loss: 3.7712\n",
      "Epoch 6/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.5796 - val_loss: 3.7558\n",
      "Epoch 7/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.5540 - val_loss: 3.7440\n",
      "Epoch 8/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.5331 - val_loss: 3.7308\n",
      "Epoch 9/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.5038 - val_loss: 3.6959\n",
      "Epoch 10/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.4751 - val_loss: 3.6753\n",
      "Epoch 11/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.4562 - val_loss: 3.6614\n",
      "Epoch 12/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.4410 - val_loss: 3.6501\n",
      "Epoch 13/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.4281 - val_loss: 3.6390\n",
      "Epoch 14/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.4138 - val_loss: 3.6230\n",
      "Epoch 15/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.3971 - val_loss: 3.6102\n",
      "Epoch 16/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3840 - val_loss: 3.6000\n",
      "Epoch 17/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3719 - val_loss: 3.5913\n",
      "Epoch 18/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3601 - val_loss: 3.5820\n",
      "Epoch 19/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3496 - val_loss: 3.5745\n",
      "Epoch 20/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3403 - val_loss: 3.5662\n",
      "Epoch 21/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3311 - val_loss: 3.5604\n",
      "Epoch 22/500\n",
      "15168/15168 [==============================] - 403s 27ms/step - loss: 3.3221 - val_loss: 3.5507\n",
      "Epoch 23/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3134 - val_loss: 3.5470\n",
      "Epoch 24/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.3045 - val_loss: 3.5379\n",
      "Epoch 25/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2968 - val_loss: 3.5339\n",
      "Epoch 26/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2863 - val_loss: 3.5232\n",
      "Epoch 27/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2736 - val_loss: 3.5119\n",
      "Epoch 28/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2590 - val_loss: 3.4988\n",
      "Epoch 29/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2449 - val_loss: 3.4896\n",
      "Epoch 30/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2332 - val_loss: 3.4794\n",
      "Epoch 31/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2224 - val_loss: 3.4691\n",
      "Epoch 32/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.2123 - val_loss: 3.4636\n",
      "Epoch 33/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.2026 - val_loss: 3.4534\n",
      "Epoch 34/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.1931 - val_loss: 3.4458\n",
      "Epoch 35/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.1832 - val_loss: 3.4362\n",
      "Epoch 36/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.1739 - val_loss: 3.4325\n",
      "Epoch 37/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.1662 - val_loss: 3.4292\n",
      "Epoch 38/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.1584 - val_loss: 3.4201\n",
      "Epoch 39/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.1509 - val_loss: 3.4169\n",
      "Epoch 40/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.1433 - val_loss: 3.4120\n",
      "Epoch 41/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.1378 - val_loss: 3.4074\n",
      "Epoch 42/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.1316 - val_loss: 3.4036\n",
      "Epoch 43/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.1256 - val_loss: 3.4011\n",
      "Epoch 44/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.1194 - val_loss: 3.3956\n",
      "Epoch 45/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.1128 - val_loss: 3.3905\n",
      "Epoch 46/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.1063 - val_loss: 3.3871\n",
      "Epoch 47/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.0991 - val_loss: 3.3827\n",
      "Epoch 48/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0918 - val_loss: 3.3757\n",
      "Epoch 49/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 3.0854 - val_loss: 3.3730\n",
      "Epoch 50/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0788 - val_loss: 3.3622\n",
      "Epoch 51/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0684 - val_loss: 3.3636\n",
      "Epoch 52/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0604 - val_loss: 3.3551\n",
      "Epoch 53/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 3.0523 - val_loss: 3.3486\n",
      "Epoch 54/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 3.0473 - val_loss: 3.3412\n",
      "Epoch 55/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0347 - val_loss: 3.3362\n",
      "Epoch 56/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 3.0261 - val_loss: 3.3269\n",
      "Epoch 57/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0168 - val_loss: 3.3251\n",
      "Epoch 58/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0087 - val_loss: 3.3135\n",
      "Epoch 59/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 3.0008 - val_loss: 3.3084\n",
      "Epoch 60/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.9876 - val_loss: 3.3011\n",
      "Epoch 61/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.9786 - val_loss: 3.2976\n",
      "Epoch 62/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.9686 - val_loss: 3.2858\n",
      "Epoch 63/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.9560 - val_loss: 3.2785\n",
      "Epoch 64/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.9435 - val_loss: 3.2679\n",
      "Epoch 65/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.9326 - val_loss: 3.2636\n",
      "Epoch 66/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.9210 - val_loss: 3.2603\n",
      "Epoch 67/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.9080 - val_loss: 3.2436\n",
      "Epoch 68/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.8926 - val_loss: 3.2348\n",
      "Epoch 69/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.8807 - val_loss: 3.2231\n",
      "Epoch 70/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.8625 - val_loss: 3.2124\n",
      "Epoch 71/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.8392 - val_loss: 3.1899\n",
      "Epoch 72/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.8127 - val_loss: 3.1631\n",
      "Epoch 73/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.7880 - val_loss: 3.1532\n",
      "Epoch 74/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.7669 - val_loss: 3.1360\n",
      "Epoch 75/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.7482 - val_loss: 3.1343\n",
      "Epoch 76/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.7295 - val_loss: 3.1107\n",
      "Epoch 77/500\n",
      "15168/15168 [==============================] - 402s 27ms/step - loss: 2.7129 - val_loss: 3.0975\n",
      "Epoch 78/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.6955 - val_loss: 3.0861\n",
      "Epoch 79/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.6754 - val_loss: 3.0779\n",
      "Epoch 80/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.6590 - val_loss: 3.0683\n",
      "Epoch 81/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.6415 - val_loss: 3.0605\n",
      "Epoch 82/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.6271 - val_loss: 3.0520\n",
      "Epoch 83/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.6101 - val_loss: 3.0463\n",
      "Epoch 84/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.5934 - val_loss: 3.0343\n",
      "Epoch 85/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.5770 - val_loss: 3.0223\n",
      "Epoch 86/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.5601 - val_loss: 3.0154\n",
      "Epoch 87/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.5434 - val_loss: 3.0140\n",
      "Epoch 88/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.5296 - val_loss: 3.0047\n",
      "Epoch 89/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.5117 - val_loss: 2.9968\n",
      "Epoch 90/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.4960 - val_loss: 3.0055\n",
      "Epoch 91/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.4889 - val_loss: 2.9839\n",
      "Epoch 92/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.4685 - val_loss: 2.9827\n",
      "Epoch 93/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.4540 - val_loss: 2.9759\n",
      "Epoch 94/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.4364 - val_loss: 2.9679\n",
      "Epoch 95/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.4204 - val_loss: 2.9591\n",
      "Epoch 96/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.4063 - val_loss: 2.9573\n",
      "Epoch 97/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.3936 - val_loss: 2.9501\n",
      "Epoch 98/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.3764 - val_loss: 2.9483\n",
      "Epoch 99/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.3649 - val_loss: 2.9423\n",
      "Epoch 100/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.3499 - val_loss: 2.9394\n",
      "Epoch 101/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.3341 - val_loss: 2.9383\n",
      "Epoch 102/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.3249 - val_loss: 2.9337\n",
      "Epoch 103/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 2.3090 - val_loss: 2.9305\n",
      "Epoch 104/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2967 - val_loss: 2.9256\n",
      "Epoch 105/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2781 - val_loss: 2.9198\n",
      "Epoch 106/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2625 - val_loss: 2.9242\n",
      "Epoch 107/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2525 - val_loss: 2.9214\n",
      "Epoch 108/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2378 - val_loss: 2.9147\n",
      "Epoch 109/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2235 - val_loss: 2.9122\n",
      "Epoch 110/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2108 - val_loss: 2.9112\n",
      "Epoch 111/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.2006 - val_loss: 2.9065\n",
      "Epoch 112/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.1855 - val_loss: 2.9062\n",
      "Epoch 113/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.1740 - val_loss: 2.9133\n",
      "Epoch 114/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.1588 - val_loss: 2.9072\n",
      "Epoch 115/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.1482 - val_loss: 2.9079\n",
      "Epoch 116/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.1358 - val_loss: 2.9077\n",
      "Epoch 117/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 2.1251 - val_loss: 2.9005\n",
      "Epoch 118/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.1163 - val_loss: 2.9033\n",
      "Epoch 119/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.1004 - val_loss: 2.9023\n",
      "Epoch 120/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0917 - val_loss: 2.9022\n",
      "Epoch 121/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0737 - val_loss: 2.8975\n",
      "Epoch 122/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0603 - val_loss: 2.9117\n",
      "Epoch 123/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0508 - val_loss: 2.9075\n",
      "Epoch 124/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0409 - val_loss: 2.9065\n",
      "Epoch 125/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0265 - val_loss: 2.9010\n",
      "Epoch 126/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0120 - val_loss: 2.9049\n",
      "Epoch 127/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 2.0014 - val_loss: 2.8996\n",
      "Epoch 128/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.9886 - val_loss: 2.9037\n",
      "Epoch 129/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.9787 - val_loss: 2.9081\n",
      "Epoch 130/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 1.9651 - val_loss: 2.9159\n",
      "Epoch 131/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.9575 - val_loss: 2.9081\n",
      "Epoch 132/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 1.9407 - val_loss: 2.9085\n",
      "Epoch 133/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.9268 - val_loss: 2.9058\n",
      "Epoch 134/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.9134 - val_loss: 2.9151\n",
      "Epoch 135/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.9025 - val_loss: 2.9184\n",
      "Epoch 136/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.8901 - val_loss: 2.9068\n",
      "Epoch 137/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.8783 - val_loss: 2.9165\n",
      "Epoch 138/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.8686 - val_loss: 2.9143\n",
      "Epoch 139/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.8553 - val_loss: 2.9175\n",
      "Epoch 140/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.8438 - val_loss: 2.9234\n",
      "Epoch 141/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.8375 - val_loss: 2.9308\n",
      "Epoch 142/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.8272 - val_loss: 2.9187\n",
      "Epoch 143/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.8110 - val_loss: 2.9269\n",
      "Epoch 144/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7993 - val_loss: 2.9283\n",
      "Epoch 145/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7882 - val_loss: 2.9323\n",
      "Epoch 146/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7765 - val_loss: 2.9232\n",
      "Epoch 147/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7786 - val_loss: 2.9323\n",
      "Epoch 148/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7615 - val_loss: 2.9381\n",
      "Epoch 149/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7448 - val_loss: 2.9326\n",
      "Epoch 150/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7354 - val_loss: 2.9472\n",
      "Epoch 151/500\n",
      "15168/15168 [==============================] - 399s 26ms/step - loss: 1.7326 - val_loss: 2.9462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7207 - val_loss: 2.9379\n",
      "Epoch 153/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7088 - val_loss: 2.9516\n",
      "Epoch 154/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6982 - val_loss: 2.9524\n",
      "Epoch 155/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6851 - val_loss: 2.9520\n",
      "Epoch 156/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6752 - val_loss: 2.9506\n",
      "Epoch 157/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6683 - val_loss: 3.0108\n",
      "Epoch 158/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.7003 - val_loss: 2.9559\n",
      "Epoch 159/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6585 - val_loss: 2.9586\n",
      "Epoch 160/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6462 - val_loss: 2.9666\n",
      "Epoch 161/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6338 - val_loss: 2.9623\n",
      "Epoch 162/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6230 - val_loss: 2.9686\n",
      "Epoch 163/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6122 - val_loss: 2.9649\n",
      "Epoch 164/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6031 - val_loss: 2.9750\n",
      "Epoch 165/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5943 - val_loss: 2.9791\n",
      "Epoch 166/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5861 - val_loss: 2.9746\n",
      "Epoch 167/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6261 - val_loss: 3.0239\n",
      "Epoch 168/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6655 - val_loss: 3.0192\n",
      "Epoch 169/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6204 - val_loss: 3.0167\n",
      "Epoch 170/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.6022 - val_loss: 3.0192\n",
      "Epoch 171/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5919 - val_loss: 3.0213\n",
      "Epoch 172/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5772 - val_loss: 3.0140\n",
      "Epoch 173/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5641 - val_loss: 3.0208\n",
      "Epoch 174/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5537 - val_loss: 3.0245\n",
      "Epoch 175/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5485 - val_loss: 3.0285\n",
      "Epoch 176/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5372 - val_loss: 3.0348\n",
      "Epoch 177/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5265 - val_loss: 3.0330\n",
      "Epoch 178/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5214 - val_loss: 3.0322\n",
      "Epoch 179/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5100 - val_loss: 3.0359\n",
      "Epoch 180/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.5021 - val_loss: 3.0435\n",
      "Epoch 181/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4942 - val_loss: 3.0479\n",
      "Epoch 182/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4849 - val_loss: 3.0446\n",
      "Epoch 183/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4768 - val_loss: 3.0601\n",
      "Epoch 184/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4685 - val_loss: 3.0599\n",
      "Epoch 185/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4640 - val_loss: 3.0630\n",
      "Epoch 186/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 1.4546 - val_loss: 3.0636\n",
      "Epoch 187/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.4489 - val_loss: 3.0648\n",
      "Epoch 188/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4450 - val_loss: 3.0648\n",
      "Epoch 189/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.4311 - val_loss: 3.0687\n",
      "Epoch 190/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.4234 - val_loss: 3.0732\n",
      "Epoch 191/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4246 - val_loss: 3.0807\n",
      "Epoch 192/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4195 - val_loss: 3.0728\n",
      "Epoch 193/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.4027 - val_loss: 3.0871\n",
      "Epoch 194/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3921 - val_loss: 3.0938\n",
      "Epoch 195/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.3869 - val_loss: 3.0831\n",
      "Epoch 196/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3806 - val_loss: 3.0876\n",
      "Epoch 197/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3730 - val_loss: 3.1050\n",
      "Epoch 198/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.3653 - val_loss: 3.1046\n",
      "Epoch 199/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3583 - val_loss: 3.1052\n",
      "Epoch 200/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3506 - val_loss: 3.1171\n",
      "Epoch 201/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.3421 - val_loss: 3.1122\n",
      "Epoch 202/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3404 - val_loss: 3.1118\n",
      "Epoch 203/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3288 - val_loss: 3.1186\n",
      "Epoch 204/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3237 - val_loss: 3.1355\n",
      "Epoch 205/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3183 - val_loss: 3.1277\n",
      "Epoch 206/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.3122 - val_loss: 3.1398\n",
      "Epoch 207/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.3030 - val_loss: 3.1315\n",
      "Epoch 208/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.2941 - val_loss: 3.1397\n",
      "Epoch 209/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.2912 - val_loss: 3.1492\n",
      "Epoch 210/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2875 - val_loss: 3.1608\n",
      "Epoch 211/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.2789 - val_loss: 3.1513\n",
      "Epoch 212/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2694 - val_loss: 3.1546\n",
      "Epoch 213/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2647 - val_loss: 3.1668\n",
      "Epoch 214/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2572 - val_loss: 3.1615\n",
      "Epoch 215/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.2535 - val_loss: 3.1717\n",
      "Epoch 216/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2462 - val_loss: 3.1747\n",
      "Epoch 217/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2418 - val_loss: 3.1846\n",
      "Epoch 218/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2416 - val_loss: 3.1830\n",
      "Epoch 219/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2348 - val_loss: 3.1806\n",
      "Epoch 220/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2258 - val_loss: 3.1899\n",
      "Epoch 221/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.2161 - val_loss: 3.1940\n",
      "Epoch 222/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.2056 - val_loss: 3.2057\n",
      "Epoch 223/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.2051 - val_loss: 3.1962\n",
      "Epoch 224/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1997 - val_loss: 3.2035\n",
      "Epoch 225/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.1997 - val_loss: 3.2034\n",
      "Epoch 226/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1858 - val_loss: 3.2087\n",
      "Epoch 227/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.1766 - val_loss: 3.2204\n",
      "Epoch 228/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1759 - val_loss: 3.2168\n",
      "Epoch 229/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1684 - val_loss: 3.2221\n",
      "Epoch 230/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.1606 - val_loss: 3.2260\n",
      "Epoch 231/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1539 - val_loss: 3.2343\n",
      "Epoch 232/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1498 - val_loss: 3.2336\n",
      "Epoch 233/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1491 - val_loss: 3.2305\n",
      "Epoch 234/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1512 - val_loss: 3.2360\n",
      "Epoch 235/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1388 - val_loss: 3.2438\n",
      "Epoch 236/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1292 - val_loss: 3.2478\n",
      "Epoch 237/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1215 - val_loss: 3.2588\n",
      "Epoch 238/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1163 - val_loss: 3.2627\n",
      "Epoch 239/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1111 - val_loss: 3.2615\n",
      "Epoch 240/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.1080 - val_loss: 3.2731\n",
      "Epoch 241/500\n",
      "15168/15168 [==============================] - 403s 27ms/step - loss: 1.1009 - val_loss: 3.2697\n",
      "Epoch 242/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0946 - val_loss: 3.2793\n",
      "Epoch 243/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0894 - val_loss: 3.2801\n",
      "Epoch 244/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0928 - val_loss: 3.2798\n",
      "Epoch 245/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0850 - val_loss: 3.2882\n",
      "Epoch 246/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0791 - val_loss: 3.3127\n",
      "Epoch 247/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0758 - val_loss: 3.3026\n",
      "Epoch 248/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0669 - val_loss: 3.3067\n",
      "Epoch 249/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0610 - val_loss: 3.3134\n",
      "Epoch 250/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0683 - val_loss: 3.3072\n",
      "Epoch 251/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0571 - val_loss: 3.3198\n",
      "Epoch 252/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0494 - val_loss: 3.3199\n",
      "Epoch 253/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0389 - val_loss: 3.3200\n",
      "Epoch 254/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0310 - val_loss: 3.3307\n",
      "Epoch 255/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0283 - val_loss: 3.3225\n",
      "Epoch 256/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0212 - val_loss: 3.3501\n",
      "Epoch 257/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0274 - val_loss: 3.3424\n",
      "Epoch 258/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0293 - val_loss: 3.3496\n",
      "Epoch 259/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 1.0146 - val_loss: 3.3536\n",
      "Epoch 260/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 1.0043 - val_loss: 3.3581\n",
      "Epoch 261/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9979 - val_loss: 3.3590\n",
      "Epoch 262/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9920 - val_loss: 3.3640\n",
      "Epoch 263/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9896 - val_loss: 3.3668\n",
      "Epoch 264/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9827 - val_loss: 3.3791\n",
      "Epoch 265/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9783 - val_loss: 3.3726\n",
      "Epoch 266/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9794 - val_loss: 3.3856\n",
      "Epoch 267/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9788 - val_loss: 3.3876\n",
      "Epoch 268/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 0.9702 - val_loss: 3.3949\n",
      "Epoch 269/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9617 - val_loss: 3.3983\n",
      "Epoch 270/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 0.9579 - val_loss: 3.4056\n",
      "Epoch 271/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9606 - val_loss: 3.4055\n",
      "Epoch 272/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9517 - val_loss: 3.4030\n",
      "Epoch 273/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9439 - val_loss: 3.4028\n",
      "Epoch 274/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9419 - val_loss: 3.4330\n",
      "Epoch 275/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9347 - val_loss: 3.4394\n",
      "Epoch 276/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9329 - val_loss: 3.4295\n",
      "Epoch 277/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9235 - val_loss: 3.4407\n",
      "Epoch 278/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9222 - val_loss: 3.4410\n",
      "Epoch 279/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9267 - val_loss: 3.4457\n",
      "Epoch 280/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9302 - val_loss: 3.4497\n",
      "Epoch 281/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9156 - val_loss: 3.4422\n",
      "Epoch 282/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9067 - val_loss: 3.4534\n",
      "Epoch 283/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9188 - val_loss: 3.4482\n",
      "Epoch 284/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.9144 - val_loss: 3.4541\n",
      "Epoch 285/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.9016 - val_loss: 3.4646\n",
      "Epoch 286/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8912 - val_loss: 3.4632\n",
      "Epoch 287/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.8861 - val_loss: 3.4775\n",
      "Epoch 288/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8764 - val_loss: 3.4794\n",
      "Epoch 289/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8802 - val_loss: 3.4835\n",
      "Epoch 290/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8740 - val_loss: 3.4867\n",
      "Epoch 291/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8690 - val_loss: 3.5035\n",
      "Epoch 292/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8656 - val_loss: 3.4946\n",
      "Epoch 293/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.8604 - val_loss: 3.5051\n",
      "Epoch 294/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8577 - val_loss: 3.5161\n",
      "Epoch 295/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8521 - val_loss: 3.5166\n",
      "Epoch 296/500\n",
      "15168/15168 [==============================] - 403s 27ms/step - loss: 0.8478 - val_loss: 3.5184\n",
      "Epoch 297/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8429 - val_loss: 3.5431\n",
      "Epoch 298/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.8397 - val_loss: 3.5439\n",
      "Epoch 299/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.8371 - val_loss: 3.5326\n",
      "Epoch 300/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8405 - val_loss: 3.5369\n",
      "Epoch 301/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8387 - val_loss: 3.5446\n",
      "Epoch 302/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.8328 - val_loss: 3.5562\n",
      "Epoch 303/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8198 - val_loss: 3.5573\n",
      "Epoch 304/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8234 - val_loss: 3.5505\n",
      "Epoch 305/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8208 - val_loss: 3.5702\n",
      "Epoch 306/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8167 - val_loss: 3.5664\n",
      "Epoch 307/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.8105 - val_loss: 3.5700\n",
      "Epoch 308/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8046 - val_loss: 3.5800\n",
      "Epoch 309/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8033 - val_loss: 3.5747\n",
      "Epoch 310/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7958 - val_loss: 3.5902\n",
      "Epoch 311/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7920 - val_loss: 3.5879\n",
      "Epoch 312/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7936 - val_loss: 3.5966\n",
      "Epoch 313/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.8124 - val_loss: 3.5853\n",
      "Epoch 314/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7983 - val_loss: 3.5839\n",
      "Epoch 315/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7953 - val_loss: 3.6038\n",
      "Epoch 316/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7786 - val_loss: 3.5935\n",
      "Epoch 317/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7688 - val_loss: 3.6177\n",
      "Epoch 318/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7630 - val_loss: 3.6205\n",
      "Epoch 319/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7582 - val_loss: 3.6285\n",
      "Epoch 320/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7907 - val_loss: 3.6257\n",
      "Epoch 321/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7779 - val_loss: 3.6138\n",
      "Epoch 322/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7791 - val_loss: 3.6356\n",
      "Epoch 323/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7604 - val_loss: 3.6405\n",
      "Epoch 324/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7518 - val_loss: 3.6429\n",
      "Epoch 325/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7512 - val_loss: 3.6381\n",
      "Epoch 326/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7374 - val_loss: 3.6516\n",
      "Epoch 327/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7353 - val_loss: 3.6560\n",
      "Epoch 328/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7301 - val_loss: 3.6624\n",
      "Epoch 329/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7268 - val_loss: 3.6737\n",
      "Epoch 330/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7222 - val_loss: 3.6655\n",
      "Epoch 331/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7186 - val_loss: 3.6775\n",
      "Epoch 332/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7242 - val_loss: 3.6854\n",
      "Epoch 333/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7201 - val_loss: 3.6992\n",
      "Epoch 334/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7229 - val_loss: 3.6986\n",
      "Epoch 335/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7111 - val_loss: 3.7027\n",
      "Epoch 336/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7069 - val_loss: 3.7112\n",
      "Epoch 337/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7044 - val_loss: 3.7188\n",
      "Epoch 338/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7005 - val_loss: 3.7188\n",
      "Epoch 339/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7021 - val_loss: 3.7249\n",
      "Epoch 340/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.7057 - val_loss: 3.7220\n",
      "Epoch 341/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6966 - val_loss: 3.7275\n",
      "Epoch 342/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7346 - val_loss: 3.7121\n",
      "Epoch 343/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.7030 - val_loss: 3.7226\n",
      "Epoch 344/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6899 - val_loss: 3.7373\n",
      "Epoch 345/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6929 - val_loss: 3.7365\n",
      "Epoch 346/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6836 - val_loss: 3.7455\n",
      "Epoch 347/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6763 - val_loss: 3.7468\n",
      "Epoch 348/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6709 - val_loss: 3.7456\n",
      "Epoch 349/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6684 - val_loss: 3.7586\n",
      "Epoch 350/500\n",
      "15168/15168 [==============================] - 402s 27ms/step - loss: 0.6702 - val_loss: 3.7579\n",
      "Epoch 351/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6627 - val_loss: 3.7656\n",
      "Epoch 352/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6555 - val_loss: 3.7706\n",
      "Epoch 353/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6562 - val_loss: 3.7680\n",
      "Epoch 354/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6603 - val_loss: 3.7828\n",
      "Epoch 355/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6590 - val_loss: 3.7801\n",
      "Epoch 356/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6593 - val_loss: 3.7830\n",
      "Epoch 357/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6511 - val_loss: 3.7868\n",
      "Epoch 358/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6454 - val_loss: 3.7969\n",
      "Epoch 359/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6426 - val_loss: 3.8085\n",
      "Epoch 360/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6462 - val_loss: 3.8067\n",
      "Epoch 361/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6340 - val_loss: 3.7990\n",
      "Epoch 362/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6363 - val_loss: 3.8058\n",
      "Epoch 363/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6397 - val_loss: 3.8099\n",
      "Epoch 364/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6375 - val_loss: 3.8199\n",
      "Epoch 365/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6256 - val_loss: 3.8313\n",
      "Epoch 366/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6233 - val_loss: 3.8245\n",
      "Epoch 367/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6156 - val_loss: 3.8398\n",
      "Epoch 368/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6158 - val_loss: 3.8392\n",
      "Epoch 369/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6130 - val_loss: 3.8496\n",
      "Epoch 370/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6085 - val_loss: 3.8613\n",
      "Epoch 371/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6178 - val_loss: 3.8459\n",
      "Epoch 372/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6300 - val_loss: 3.8574\n",
      "Epoch 373/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6116 - val_loss: 3.8608\n",
      "Epoch 374/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.6080 - val_loss: 3.8726\n",
      "Epoch 375/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.6003 - val_loss: 3.8697\n",
      "Epoch 376/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5906 - val_loss: 3.8869\n",
      "Epoch 377/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5856 - val_loss: 3.8790\n",
      "Epoch 378/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5931 - val_loss: 3.8818\n",
      "Epoch 379/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5854 - val_loss: 3.8893\n",
      "Epoch 380/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5817 - val_loss: 3.8998\n",
      "Epoch 381/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5785 - val_loss: 3.9082\n",
      "Epoch 382/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5762 - val_loss: 3.9081\n",
      "Epoch 383/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5892 - val_loss: 3.8946\n",
      "Epoch 384/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5782 - val_loss: 3.9121\n",
      "Epoch 385/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5847 - val_loss: 3.9179\n",
      "Epoch 386/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5741 - val_loss: 3.9362\n",
      "Epoch 387/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5730 - val_loss: 3.9378\n",
      "Epoch 388/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5650 - val_loss: 3.9471\n",
      "Epoch 389/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5618 - val_loss: 3.9336\n",
      "Epoch 390/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5563 - val_loss: 3.9420\n",
      "Epoch 391/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5537 - val_loss: 3.9494\n",
      "Epoch 392/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5502 - val_loss: 3.9617\n",
      "Epoch 393/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5457 - val_loss: 3.9565\n",
      "Epoch 394/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5443 - val_loss: 3.9695\n",
      "Epoch 395/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5516 - val_loss: 3.9683\n",
      "Epoch 396/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5428 - val_loss: 3.9821\n",
      "Epoch 397/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5388 - val_loss: 3.9723\n",
      "Epoch 398/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5415 - val_loss: 3.9754\n",
      "Epoch 399/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5375 - val_loss: 3.9781\n",
      "Epoch 400/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5329 - val_loss: 3.9810\n",
      "Epoch 401/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5356 - val_loss: 3.9972\n",
      "Epoch 402/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5416 - val_loss: 4.0049\n",
      "Epoch 403/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5571 - val_loss: 4.0019\n",
      "Epoch 404/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5401 - val_loss: 3.9962\n",
      "Epoch 405/500\n",
      "15168/15168 [==============================] - 402s 27ms/step - loss: 0.5350 - val_loss: 3.9948\n",
      "Epoch 406/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5202 - val_loss: 4.0168\n",
      "Epoch 407/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5118 - val_loss: 4.0180\n",
      "Epoch 408/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5122 - val_loss: 4.0342\n",
      "Epoch 409/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5189 - val_loss: 4.0325\n",
      "Epoch 410/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5095 - val_loss: 4.0386\n",
      "Epoch 411/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5124 - val_loss: 4.0418\n",
      "Epoch 412/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5104 - val_loss: 4.0390\n",
      "Epoch 413/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5048 - val_loss: 4.0550\n",
      "Epoch 414/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5269 - val_loss: 4.0241\n",
      "Epoch 415/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5108 - val_loss: 4.0407\n",
      "Epoch 416/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.5040 - val_loss: 4.0515\n",
      "Epoch 417/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4936 - val_loss: 4.0619\n",
      "Epoch 418/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4887 - val_loss: 4.0599\n",
      "Epoch 419/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4893 - val_loss: 4.0712\n",
      "Epoch 420/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.4880 - val_loss: 4.0773\n",
      "Epoch 421/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4878 - val_loss: 4.0891\n",
      "Epoch 422/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4812 - val_loss: 4.0885\n",
      "Epoch 423/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4960 - val_loss: 4.0925\n",
      "Epoch 424/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4956 - val_loss: 4.0895\n",
      "Epoch 425/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.4978 - val_loss: 4.1095\n",
      "Epoch 426/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 0.4842 - val_loss: 4.0952\n",
      "Epoch 427/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4736 - val_loss: 4.1093\n",
      "Epoch 428/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4786 - val_loss: 4.1070\n",
      "Epoch 429/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4748 - val_loss: 4.1113\n",
      "Epoch 430/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4713 - val_loss: 4.1232\n",
      "Epoch 431/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4732 - val_loss: 4.1152\n",
      "Epoch 432/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4626 - val_loss: 4.1210\n",
      "Epoch 433/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4550 - val_loss: 4.1267\n",
      "Epoch 434/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4529 - val_loss: 4.1428\n",
      "Epoch 435/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4546 - val_loss: 4.1432\n",
      "Epoch 436/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4531 - val_loss: 4.1493\n",
      "Epoch 437/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4507 - val_loss: 4.1518\n",
      "Epoch 438/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4456 - val_loss: 4.1542\n",
      "Epoch 439/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4477 - val_loss: 4.1555\n",
      "Epoch 440/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4500 - val_loss: 4.1729\n",
      "Epoch 441/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4460 - val_loss: 4.1664\n",
      "Epoch 442/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4522 - val_loss: 4.1932\n",
      "Epoch 443/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4526 - val_loss: 4.1697\n",
      "Epoch 444/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4463 - val_loss: 4.1846\n",
      "Epoch 445/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4478 - val_loss: 4.1816\n",
      "Epoch 446/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4453 - val_loss: 4.1753\n",
      "Epoch 447/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4462 - val_loss: 4.1949\n",
      "Epoch 448/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4358 - val_loss: 4.2040\n",
      "Epoch 449/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4355 - val_loss: 4.1973\n",
      "Epoch 450/500\n",
      "15168/15168 [==============================] - 400s 26ms/step - loss: 0.4346 - val_loss: 4.2152\n",
      "Epoch 451/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4312 - val_loss: 4.1941\n",
      "Epoch 452/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4226 - val_loss: 4.2065\n",
      "Epoch 453/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4380 - val_loss: 4.2252\n",
      "Epoch 454/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4281 - val_loss: 4.2134\n",
      "Epoch 455/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4293 - val_loss: 4.2201\n",
      "Epoch 456/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5775 - val_loss: 4.2080\n",
      "Epoch 457/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.5358 - val_loss: 4.1831\n",
      "Epoch 458/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4672 - val_loss: 4.2022\n",
      "Epoch 459/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4512 - val_loss: 4.2060\n",
      "Epoch 460/500\n",
      "15168/15168 [==============================] - 403s 27ms/step - loss: 0.4448 - val_loss: 4.2109\n",
      "Epoch 461/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4206 - val_loss: 4.2092\n",
      "Epoch 462/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4088 - val_loss: 4.2316\n",
      "Epoch 463/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4038 - val_loss: 4.2292\n",
      "Epoch 464/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3979 - val_loss: 4.2455\n",
      "Epoch 465/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3996 - val_loss: 4.2590\n",
      "Epoch 466/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3963 - val_loss: 4.2672\n",
      "Epoch 467/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3899 - val_loss: 4.2641\n",
      "Epoch 468/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3959 - val_loss: 4.2676\n",
      "Epoch 469/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3907 - val_loss: 4.2812\n",
      "Epoch 470/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3856 - val_loss: 4.2769\n",
      "Epoch 471/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3876 - val_loss: 4.2899\n",
      "Epoch 472/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3876 - val_loss: 4.2901\n",
      "Epoch 473/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3808 - val_loss: 4.3042\n",
      "Epoch 474/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3825 - val_loss: 4.3019\n",
      "Epoch 475/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3827 - val_loss: 4.2946\n",
      "Epoch 476/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3850 - val_loss: 4.3168\n",
      "Epoch 477/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4031 - val_loss: 4.3097\n",
      "Epoch 478/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3947 - val_loss: 4.3083\n",
      "Epoch 479/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3924 - val_loss: 4.3270\n",
      "Epoch 480/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3866 - val_loss: 4.3183\n",
      "Epoch 481/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4048 - val_loss: 4.3080\n",
      "Epoch 482/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3994 - val_loss: 4.3060\n",
      "Epoch 483/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 0.3967 - val_loss: 4.3237\n",
      "Epoch 484/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3753 - val_loss: 4.3233\n",
      "Epoch 485/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3886 - val_loss: 4.3325\n",
      "Epoch 486/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.4311 - val_loss: 4.3212\n",
      "Epoch 487/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3861 - val_loss: 4.3271\n",
      "Epoch 488/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3700 - val_loss: 4.3432\n",
      "Epoch 489/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3604 - val_loss: 4.3412\n",
      "Epoch 490/500\n",
      "15168/15168 [==============================] - 402s 26ms/step - loss: 0.3627 - val_loss: 4.3523\n",
      "Epoch 491/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3596 - val_loss: 4.3692\n",
      "Epoch 492/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3572 - val_loss: 4.3588\n",
      "Epoch 493/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3523 - val_loss: 4.3692\n",
      "Epoch 494/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3463 - val_loss: 4.3798\n",
      "Epoch 495/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3439 - val_loss: 4.3822\n",
      "Epoch 496/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3432 - val_loss: 4.3779\n",
      "Epoch 497/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3430 - val_loss: 4.3924\n",
      "Epoch 498/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3465 - val_loss: 4.3965\n",
      "Epoch 499/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3454 - val_loss: 4.4190\n",
      "Epoch 500/500\n",
      "15168/15168 [==============================] - 401s 26ms/step - loss: 0.3422 - val_loss: 4.4048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc9c9c92ba8>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_split=(1./3),\n",
    "#           verbose=1, callbacks=[checkpoint, csvlogger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Train/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('./300_word2vec_best_model/weights.0121-2.897.h5')\n",
    "\n",
    "encoder_inputs = model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,), name='input_5')\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,), name='input_6')\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_lstm = model.layers[3]\n",
    "\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "                    [decoder_inputs] + decoder_states_inputs,\n",
    "                    [decoder_outputs] + decoder_states\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 81\n",
    "max_decoder_seq_length = 6\n",
    "\n",
    "X_te_padded = pad_sequences(tokenizer.texts_to_sequences(X_test_sequence), maxlen=81, padding='post', truncating='post')\n",
    "y_te_padded = pad_sequences(tokenizer.texts_to_sequences(y_test_target), maxlen=6, padding='post', truncating='post')\n",
    "\n",
    "test_encoder_input_data = np.zeros(\n",
    "    (len(X_test_sequence), max_encoder_seq_length, EMBEDDING_DIM),\n",
    "    dtype='float32')\n",
    "test_decoder_input_data = np.zeros(\n",
    "    (len(y_test_target), max_decoder_seq_length, EMBEDDING_DIM),\n",
    "    dtype='float32')\n",
    "test_decoder_target_data = np.zeros(\n",
    "    (len(y_test_target), max_decoder_seq_length, len(tokenizer.word_index)),\n",
    "    dtype='float32')\n",
    "\n",
    "test_sequence = []\n",
    "test_target_sequence = []\n",
    "\n",
    "for sample in X_test_sequence:\n",
    "    test_sequence.append(sample.split())\n",
    "for target in y_test_target:\n",
    "    test_target_sequence.append(target.split())\n",
    "    \n",
    "# 100-dim -> input sequence, input decoder\n",
    "# 42K-dim -> output sequence.\n",
    "\n",
    "for i, (input_text, target_text, target_padded) in enumerate(zip(test_sequence, test_target_sequence, y_te_padded)):\n",
    "    for t, word in enumerate(input_text):\n",
    "        try:\n",
    "            test_encoder_input_data[i, t, :] = w2v_model[word]\n",
    "        except KeyError as error:\n",
    "            continue\n",
    "    \n",
    "    for t, word in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        try:\n",
    "            test_decoder_input_data[i, t, :] = w2v_model[word]\n",
    "        except KeyError as error:\n",
    "            continue\n",
    "        \n",
    "    for t, word in enumerate(target_padded):\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            test_decoder_target_data[i, t - 1, word] = 1.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict((i,word) for word,i in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_list(X):\n",
    "    y_candidate = []\n",
    "    \n",
    "    for i in range(X.shape[0]-1-5):\n",
    "        y_candidate.append(X[i:i+5])\n",
    "    \n",
    "    return np.asarray(y_candidate)\n",
    "\n",
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "\n",
    "def target_index(doc_idx, candidate_seq, y):\n",
    "    for i,j in enumerate(candidate_seq):\n",
    "        if len(intersection(j, y)) == len(y):\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "# doc num, doc index argmax\n",
    "\n",
    "def to_sequence(int_sequence):\n",
    "    decoded = ''\n",
    "    for i,intnum in enumerate(int_sequence):\n",
    "        if intnum == 0:\n",
    "            word = '<PAD>'\n",
    "        else:\n",
    "            word = reverse_word_index[intnum]\n",
    "        \n",
    "        if i == len(int_sequence):\n",
    "            decoded += word\n",
    "        else:\n",
    "            decoded += word + ' '\n",
    "    return decoded\n",
    "\n",
    "def rouge_one(true, candidate, start_index):\n",
    "    \n",
    "    if isinstance(true, str) and isinstance(candidate, str):\n",
    "        true = true.split()\n",
    "        candidate = candidate.split()\n",
    "    \n",
    "    overlap = [value for value in true[start_index:] if value in candidate[start_index:]] \n",
    "\n",
    "    \n",
    "    if len(true[start_index:]) != 0:\n",
    "        recall = len(overlap)/len(true[start_index:])\n",
    "    else:\n",
    "        recall = 0\n",
    "    \n",
    "    if len(candidate[start_index:]):\n",
    "        precision = len(overlap)/len(candidate[start_index:])\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    if (recall+precision) != 0:    \n",
    "        f1 = 2*((recall*precision)/(recall+precision))\n",
    "    else:\n",
    "        f1 = 0\n",
    "    \n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with candidate\n",
    "\n",
    "def test_decode_sequence_target(candidate_states_value, candidate_target_seq):\n",
    "#     candidate_states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    from_candidate_target_seq = np.zeros((1,1, EMBEDDING_DIM))\n",
    "    \n",
    "    candidate_token_index = candidate_target_seq[0,0]\n",
    "    try:\n",
    "        from_candidate_target_seq[0,0,:] = w2v_model[index_to_word[candidate_token_index]]\n",
    "    except KeyError as error:\n",
    "        pass\n",
    "    \n",
    "    candidate_joint_log_prob = 0\n",
    "    \n",
    "    for i in range(1,5):\n",
    "        from_candidate_output_tokens, h_true, c_true = decoder_model.predict([from_candidate_target_seq] + candidate_states_value)\n",
    "    \n",
    "        candidate_target_prob = from_candidate_output_tokens[0,-1, candidate_target_seq[0,i]]\n",
    "        candidate_joint_log_prob += np.log(candidate_target_prob)\n",
    "        \n",
    "        # get the t+1 input\n",
    "        \n",
    "        candidate_token_index = candidate_target_seq[0,i]\n",
    "        from_candidate_target_seq = np.zeros((1,1,EMBEDDING_DIM))\n",
    "        try:\n",
    "            from_candidate_target_seq[0,0,:] = w2v_model[index_to_word[candidate_token_index]]\n",
    "        except KeyError as error:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        candidate_states_value = [h_true, c_true]\n",
    "\n",
    "    return candidate_joint_log_prob, candidate_target_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "file = open(\"candidate_jll_300_word2vec_imdb_test.csv\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove 100 test: processing document 0\n",
      "glove 100 test: processing document 1000\n",
      "glove 100 test: processing document 2000\n",
      "glove 100 test: processing document 3000\n",
      "glove 100 test: processing document 4000\n",
      "glove 100 test: processing document 5000\n",
      "glove 100 test: processing document 6000\n",
      "glove 100 test: processing document 7000\n",
      "glove 100 test: processing document 8000\n",
      "glove 100 test: processing document 9000\n",
      "glove 100 test: processing document 10000\n",
      "glove 100 test: processing document 11000\n",
      "glove 100 test: processing document 12000\n",
      "glove 100 test: processing document 13000\n",
      "glove 100 test: processing document 14000\n",
      "glove 100 test: processing document 15000\n",
      "glove 100 test: processing document 16000\n",
      "glove 100 test: processing document 17000\n",
      "glove 100 test: processing document 18000\n",
      "glove 100 test: processing document 19000\n",
      "glove 100 test: processing document 20000\n",
      "glove 100 test: processing document 21000\n",
      "glove 100 test: processing document 22000\n"
     ]
    }
   ],
   "source": [
    "for i,doc in enumerate(X_te_padded):\n",
    "    y_candidate = generate_candidate_list(doc)\n",
    "    \n",
    "    candidate_jll_per_doc = []\n",
    "    input_seq = test_encoder_input_data[i:i+1]\n",
    "    \n",
    "    true_target_index = target_index(i, y_candidate, y_te_padded[i])\n",
    "    \n",
    "    # Encode\n",
    "    candidate_states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    for j in range(y_candidate.shape[0]):\n",
    "        candidate_seq = y_candidate[j:j+1]\n",
    "        candidate_jll_slide, candidate_last_prob = test_decode_sequence_target(candidate_states_value, candidate_seq)\n",
    "        candidate_jll_per_doc.append(candidate_jll_slide)\n",
    "   \n",
    "    candidate_jll_per_doc = np.asarray(candidate_jll_per_doc)\n",
    "    max_jll_index = np.argmax(candidate_jll_per_doc)\n",
    "    true_target_jll = np.around(candidate_jll_per_doc[true_target_index],5)\n",
    "    max_candidate_jll = np.around(candidate_jll_per_doc[max_jll_index],5)\n",
    "    \n",
    "    [precision, recall, f_score] = rouge_one(y_test_target[i], to_sequence(y_candidate[max_jll_index]), 1)\n",
    "    \n",
    "    file.write('%d\\t%d\\t%s\\t%d\\t%s\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\n' %(i, true_target_index, y_test_target[i],\n",
    "                                                            max_jll_index, to_sequence(y_candidate[max_jll_index]),\n",
    "                                                            -(true_target_index-max_jll_index),\n",
    "                                                            true_target_jll, max_candidate_jll,\n",
    "                                                            np.absolute(true_target_jll-max_candidate_jll),\n",
    "                                                            len(intersection(y_te_padded[i], y_candidate[max_jll_index])),\n",
    "                                                            np.exp(true_target_jll/4), np.exp(max_candidate_jll/4),\n",
    "                                                            precision, recall, f_score))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     print('%d\\t%d\\t%s\\t%d\\t%s\\t%d\\t%.5f\\t%.5f\\t%.5f\\t%d\\n' %(i, true_target_index, y['text'][i],\n",
    "#                                                             max_jll_index, to_sequence(y_candidate[max_jll_index]),\n",
    "#                                                             -(true_target_index-max_jll_index),\n",
    "#                                                             true_target_jll, max_candidate_jll,\n",
    "#                                                             np.absolute(true_target_jll-max_candidate_jll),\n",
    "#                                                             len(intersection(y['padded'][i], y_candidate[max_jll_index]))))\n",
    "    if i % 1000 == 0:\n",
    "#         print('Processing document %d...' %(i))\n",
    "        msg = 'glove 100 test: processing document ' + str(i)\n",
    "        u.slack_post_message(slack, msg, 'deep-learning', 'test dataset')\n",
    "        print(msg)\n",
    "        \n",
    "#     i += 1\n",
    "    \n",
    "file.close()\n",
    "# report_stats('Processing DONE', 'deep-learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u.slack_post_message(slack, msg, 'deep-learning', 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
